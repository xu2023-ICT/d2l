# 第5章 深度学习计算

## 5.1 层和块

### 练习 5.1.1 

如果将MySequential中存储块的方式更改为Python列表，会出现什么样的问题？ 

**解答：** 

&emsp;&emsp;如果将MySequential中存储块的方式从OrderedDict更改为Python列表,代码可以正常计算。但无法像`_modules`一样使用`net.state_dict()`方便的访问模型的网络结构和参数。


```python
import torch.nn as nn
import torch

class MySequential(nn.Module):
    def __init__(self, *args):
        super().__init__()
        for idx, module in enumerate(args):
            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员
            # 变量_modules中。_module的类型是OrderedDict
            self._modules[str(idx)] = module

    def forward(self, X):
        # OrderedDict保证了按照成员添加的顺序遍历它们
        for block in self._modules.values():
            X = block(X)
        return X

class MySequential_list(nn.Module):
    # 使用list
    def __init__(self, *args):
        super(MySequential_list, self).__init__()
        self.sequential = []
        for module in args:
            self.sequential.append(module)

    def forward(self, X):
        for module in self.sequential:
            X = module(X)
        return X


X = torch.rand(1,10)
net = MySequential(nn.Linear(10, 20), nn.ReLU(), nn.Linear(20, 10))
net_list = MySequential_list(nn.Linear(10, 20), nn.ReLU(), nn.Linear(20, 10))
# 结果一样
print(net(X))
print(net_list(X))
# 使用_modules方便打印net的网络结构和参数，而list则无法做到
print(net, '\n', net.state_dict())
print(net_list, '\n', net_list.state_dict())
```

    tensor([[ 0.0822, -0.1590,  0.3334,  0.5160,  0.0290,  0.2195,  0.0100, -0.1426,
             -0.2757,  0.0305]], grad_fn=<AddmmBackward0>)
    tensor([[-1.1942e-01, -9.6460e-02, -2.7601e-01, -1.5968e-01, -1.1686e-01,
              2.4949e-01,  1.4880e-01, -1.9188e-04, -2.8319e-02,  2.5360e-01]],
           grad_fn=<AddmmBackward0>)
    MySequential(
      (0): Linear(in_features=10, out_features=20, bias=True)
      (1): ReLU()
      (2): Linear(in_features=20, out_features=10, bias=True)
    ) 
     OrderedDict([('0.weight', tensor([[-0.0159, -0.0721,  0.0345, -0.2683, -0.0870,  0.0101,  0.0377,  0.1490,
              0.2500,  0.0221],
            [-0.1784, -0.0185, -0.2567, -0.0060, -0.2049, -0.2430, -0.0439,  0.1238,
             -0.2498, -0.3140],
            [ 0.1973,  0.1610, -0.0951, -0.0050,  0.2192, -0.0419,  0.0493,  0.1120,
             -0.1944, -0.0728],
            [-0.1068, -0.1210,  0.0203, -0.1285, -0.1516,  0.0395,  0.1699,  0.1095,
             -0.1498,  0.0963],
            [-0.3096,  0.0453, -0.0816,  0.2177,  0.1166,  0.3048, -0.0891,  0.2068,
              0.2328,  0.2267],
            [ 0.0821,  0.2504, -0.0873,  0.2574,  0.3162,  0.0993,  0.1835, -0.1418,
             -0.1261, -0.0138],
            [-0.1506, -0.3137,  0.1983,  0.2221, -0.3008, -0.1465,  0.2545,  0.1994,
             -0.2081, -0.1057],
            [-0.1959, -0.0709,  0.0529,  0.3097, -0.0264,  0.2153,  0.3104,  0.2884,
             -0.1512, -0.2955],
            [ 0.2716, -0.0958,  0.2383, -0.0348, -0.0171, -0.0949,  0.2012, -0.2907,
              0.1929,  0.1713],
            [-0.0049,  0.2134, -0.1321, -0.1635, -0.3051, -0.1493, -0.0559, -0.1270,
             -0.2455, -0.2395],
            [ 0.3100, -0.1215,  0.2909,  0.2133,  0.1975, -0.1940, -0.0935,  0.0140,
             -0.2544,  0.2582],
            [-0.0716,  0.2863, -0.0544, -0.0938,  0.3091, -0.1831,  0.0854,  0.1830,
             -0.1335, -0.3051],
            [ 0.1412,  0.3076,  0.0638, -0.1089,  0.0180,  0.2572, -0.3001,  0.1987,
              0.0405,  0.1663],
            [-0.1405,  0.0016,  0.2649,  0.0699, -0.0926, -0.0967, -0.2321, -0.2091,
              0.0186, -0.3146],
            [ 0.1391,  0.2191,  0.1871,  0.0192,  0.1033,  0.1934, -0.2081, -0.1284,
              0.1665,  0.2631],
            [-0.0813, -0.1420,  0.0083,  0.3141, -0.0989, -0.1864,  0.0405, -0.1168,
              0.1896, -0.0655],
            [ 0.0833, -0.0498,  0.2223, -0.0178, -0.1713, -0.2366, -0.0249, -0.2457,
             -0.1285, -0.0058],
            [ 0.2920, -0.2619, -0.1863, -0.2542, -0.0801,  0.0413,  0.0236,  0.1786,
              0.2658, -0.3041],
            [-0.1729, -0.0945,  0.1047,  0.0583,  0.2903, -0.2030,  0.0143, -0.1866,
             -0.2270,  0.0629],
            [ 0.0658, -0.0758, -0.2035,  0.1981,  0.1900, -0.1183,  0.1472, -0.2829,
              0.0759,  0.2941]])), ('0.bias', tensor([-0.1290, -0.0154, -0.0583, -0.2360,  0.3113, -0.1354, -0.2150, -0.2286,
            -0.0964,  0.0213,  0.0878,  0.2771, -0.1431, -0.3004, -0.1898, -0.0153,
             0.0695,  0.0700, -0.1644, -0.1362])), ('2.weight', tensor([[ 0.2045, -0.0160, -0.1549,  0.1597,  0.2128,  0.1612, -0.0414, -0.0103,
              0.0970,  0.1702, -0.1074, -0.0500,  0.0703, -0.0708,  0.2070, -0.1618,
              0.1986, -0.1340, -0.0630, -0.0056],
            [-0.0521,  0.0947, -0.0158,  0.1325,  0.0528, -0.0929, -0.0549,  0.0243,
              0.0007, -0.2142,  0.1810, -0.1177, -0.0357,  0.1703,  0.0761, -0.2005,
             -0.1043, -0.0054, -0.0211,  0.2146],
            [-0.0388, -0.1602, -0.0344, -0.0357,  0.0772,  0.0263,  0.1804,  0.2091,
             -0.0846, -0.1859, -0.1996,  0.1939, -0.1468, -0.2229, -0.0644, -0.1292,
             -0.0459,  0.1968, -0.1825,  0.0852],
            [ 0.1273, -0.0381,  0.1399,  0.0193,  0.0314,  0.1677, -0.1239, -0.0553,
             -0.0246, -0.0401,  0.1710,  0.1898, -0.2031, -0.0030,  0.1949,  0.0466,
              0.1635, -0.2140,  0.1289,  0.0867],
            [-0.0198, -0.1877, -0.0885,  0.1384,  0.0551,  0.1654,  0.1269, -0.0846,
             -0.0203, -0.1484, -0.0401,  0.2185,  0.0378, -0.2043, -0.0641, -0.0560,
              0.1028, -0.1116,  0.0670,  0.1523],
            [-0.0493,  0.1663,  0.1335, -0.0739,  0.1126, -0.0299,  0.0695,  0.0505,
             -0.0973, -0.0895,  0.2085, -0.0046, -0.1319,  0.2130, -0.0386, -0.1066,
              0.2139, -0.0151,  0.0345, -0.0830],
            [-0.1197, -0.2136, -0.0140,  0.1400,  0.1051, -0.0645, -0.0747,  0.0679,
             -0.2209, -0.2090,  0.1050, -0.1265, -0.1021,  0.0025, -0.0952,  0.1447,
             -0.1373, -0.0905, -0.1445,  0.1771],
            [-0.1452,  0.1269,  0.2035,  0.0585,  0.1895, -0.1672,  0.2019, -0.0097,
              0.2059, -0.1276, -0.1455, -0.1000,  0.0103,  0.0553,  0.0539, -0.1043,
              0.1393, -0.0054, -0.1506,  0.1984],
            [ 0.1188,  0.2172, -0.0228,  0.1151, -0.2167,  0.0092,  0.0547,  0.0314,
             -0.0467,  0.0820,  0.1775, -0.0358, -0.0477, -0.2215,  0.0163, -0.0171,
             -0.0209, -0.1883,  0.1211,  0.1746],
            [-0.1085,  0.1217,  0.0867,  0.0127, -0.1328,  0.2147,  0.0677,  0.0716,
             -0.1452, -0.1269,  0.1990,  0.0357, -0.1935, -0.0784, -0.1453,  0.2082,
              0.0435,  0.1069, -0.0628,  0.1833]])), ('2.bias', tensor([-0.0767, -0.1668,  0.1875,  0.2134, -0.1747,  0.0913,  0.0118, -0.1427,
            -0.2073, -0.1263]))])
    MySequential_list() 
     OrderedDict()


### 练习 5.1.2

实现一个块，它以两个块为参数，例如net1和net2，并返回前向传播中两个网络的串联输出。这也被称为平行块。

**解答：**

&emsp;&emsp;在本书7.4节中GoogleLet模型中的Inception块使用了平行块技术。
下面代码实现了一个并行网络，由两个子网络组成。输入数据先分别经过两个子网络的计算，分别得到两个部分的输出结果，然后在通道维度上合并结果得到最终输出。

&emsp;&emsp;其中，`net1`和`net2`分别表示两个子网络，torch.cat表示在指定维度上拼接张量。输出结果的大小为torch.Size([2, 36])，其中第一个维度表示batch_size为2，第二个维度表示输出特征图的通道数为36（12+24）。


```python
import torch.nn as nn
import torch


class Parallel(nn.Module):
    def __init__(self, net1, net2):
        super().__init__()
        self.net1=net1 # 第一个子网络
        self.net2=net2 # 第二个子网络
        
    def forward(self,X):
        x1= self.net1(X) # 第一个子网络的输出
        x2= self.net2(X) # 第二个子网络的输出
        return torch.cat((x1,x2),dim=1) # 在通道维度上合并输出结果
      
X = torch.rand(2,10) # 输入数据
net = Parallel(nn.Sequential(nn.Linear(10,12),nn.ReLU()), nn.Sequential(nn.Linear(10,24),nn.ReLU())) # 实例化并行网络
output = net(X)
output.size() # 输出结果的大小
```




    torch.Size([2, 36])



### 练习 5.1.3

假设我们想要连接同一网络的多个实例。实现一个函数，该函数生成同一个块的多个实例，并在此基础上构建更大的网络。

**解答：** 

&emsp;&emsp;下面代码定义了一个函数`create_network`，该函数接受四个参数：`num_instances`、`input_size`、`hidden_size` 和 `output_size`，并返回一个Sequential模型。

&emsp;&emsp;其中，该网络模型首先包含 `num_instances` 个相同的线性层，每个线性层有两个子层：一个输入维度为 `input_size`，输出维度为 `hidden_size` 的全连接层，和一个 ReLU 非线性激活层。然后，这 `hidden_size` 个线性层连接在一起作为整个网络的前馈部分。最后，额外添加一个输出层，其输入维度为 `input_size`，输出维度为 `output_size`。

&emsp;&emsp;因此，最终的网络结构是由 `output_size` 个相同的线性层组成的前馈神经网络，每个线性层内部包含一个全连接层和一个ReLU激活层，以及一个独立的输出层。


```python
import torch.nn as nn
import torch

def create_network(num_instances, input_size, hidden_size, output_size):
    # 创建一个线性层
    linear_layer = nn.Sequential(
        nn.Linear(input_size, hidden_size), nn.ReLU(),
        nn.Linear(hidden_size, input_size)
    )
    
    # 创建多个实例并连接
    instances = [linear_layer for _ in range(num_instances)]
    network = nn.Sequential(*instances)
    
    # 添加输出层
    output_layer = nn.Linear(input_size, output_size)
    network.add_module("output", output_layer)
    
    return network
# 示例用法
net = create_network(num_instances=3, input_size=10, hidden_size=5, output_size=2)
net
```




    Sequential(
      (0): Sequential(
        (0): Linear(in_features=10, out_features=5, bias=True)
        (1): ReLU()
        (2): Linear(in_features=5, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=10, out_features=5, bias=True)
        (1): ReLU()
        (2): Linear(in_features=5, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=10, out_features=5, bias=True)
        (1): ReLU()
        (2): Linear(in_features=5, out_features=10, bias=True)
      )
      (output): Linear(in_features=10, out_features=2, bias=True)
    )



## 5.2 参数管理 

### 练习 5.2.1

使用`NestMLP`模型，访问各个层的参数。

 **解答：**

&emsp;&emsp;引用上5.1节中的`NestMLP`模型,可以使用以下代码访问该模型各个层的参数,输出结果将显示每个层对应的参数名称、形状和具体参数。


```python
class NestMLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),
                                 nn.Linear(64, 32), nn.ReLU())
        self.linear = nn.Linear(32, 16)

    def forward(self, X):
        return self.linear(self.net(X))

model = NestMLP()

# 访问net层的参数
print('访问net层的参数')
for name, param in model.net.named_parameters():
    print(f"Parameter name: {name}, shape:{param.shape}, Parameter: {param}")

# 访问linear层的参数
print('访问linear层的参数')
for name, param in model.linear.named_parameters():
    print(f"Parameter name: {name}, shape:{param.shape}, Parameter: {param}")
```

    访问net层的参数
    Parameter name: 0.weight, shape:torch.Size([64, 20]), Parameter: Parameter containing:
    tensor([[-0.1808,  0.1623,  0.1909,  ...,  0.0106,  0.0485, -0.1297],
            [ 0.0486,  0.0337, -0.1276,  ...,  0.1684,  0.1599, -0.1760],
            [ 0.1231,  0.0936,  0.1819,  ..., -0.1704, -0.1879,  0.0596],
            ...,
            [-0.0950,  0.1598, -0.0491,  ..., -0.0407,  0.0716, -0.0979],
            [-0.1242,  0.2079,  0.1782,  ..., -0.1273,  0.1436, -0.2166],
            [-0.2123, -0.0288,  0.1501,  ...,  0.1547,  0.0819,  0.0293]],
           requires_grad=True)
    Parameter name: 0.bias, shape:torch.Size([64]), Parameter: Parameter containing:
    tensor([-0.1261, -0.0023,  0.0281, -0.0212,  0.1164, -0.1347,  0.1767,  0.1912,
            -0.1671, -0.0260,  0.1449, -0.0187, -0.0012,  0.1228,  0.0514, -0.1662,
            -0.2011, -0.2227, -0.0638, -0.1534,  0.2052,  0.0034, -0.1758,  0.0989,
             0.0733,  0.1926, -0.0881,  0.0852,  0.1977,  0.1340,  0.0270,  0.1664,
            -0.0567,  0.0511, -0.0518,  0.1008,  0.0869,  0.0298, -0.0761,  0.0583,
            -0.1586,  0.1321, -0.0922,  0.0719,  0.1676,  0.0500, -0.1327,  0.1057,
             0.0687, -0.2095, -0.1943,  0.0591, -0.2097,  0.1565,  0.1494,  0.1002,
            -0.0552,  0.0946, -0.0186,  0.0429,  0.2049, -0.1003, -0.2212,  0.0837],
           requires_grad=True)
    Parameter name: 2.weight, shape:torch.Size([32, 64]), Parameter: Parameter containing:
    tensor([[-0.0622,  0.0184,  0.0581,  ...,  0.0973, -0.0306,  0.0617],
            [-0.0218,  0.0515,  0.0183,  ...,  0.0367,  0.0720, -0.0936],
            [-0.1094,  0.0040, -0.1073,  ..., -0.0152, -0.0694,  0.0299],
            ...,
            [ 0.0685, -0.0158, -0.0579,  ...,  0.0384, -0.0336, -0.0009],
            [ 0.0295,  0.0049,  0.0573,  ...,  0.0113, -0.0152,  0.0805],
            [-0.1228,  0.0676, -0.0697,  ..., -0.0804, -0.0315,  0.0960]],
           requires_grad=True)
    Parameter name: 2.bias, shape:torch.Size([32]), Parameter: Parameter containing:
    tensor([ 0.0332, -0.0710, -0.0883,  0.1078,  0.0642, -0.0313,  0.0398, -0.0952,
             0.0641,  0.0624,  0.0576, -0.0333,  0.0714,  0.1044, -0.0732,  0.0477,
            -0.0283,  0.0032,  0.1023, -0.0056,  0.0940,  0.0739, -0.1093, -0.0777,
             0.0723,  0.1116,  0.1102, -0.1202, -0.0717, -0.0863,  0.0949,  0.0335],
           requires_grad=True)
    访问linear层的参数
    Parameter name: weight, shape:torch.Size([16, 32]), Parameter: Parameter containing:
    tensor([[-0.0232, -0.0074,  0.1396,  0.0738, -0.0259,  0.0405, -0.1646, -0.1197,
              0.1180, -0.1047,  0.0617, -0.1469, -0.0458,  0.1325,  0.1365, -0.0445,
              0.1145,  0.0715, -0.0149,  0.0984,  0.1517, -0.1463,  0.1390, -0.1377,
              0.1494, -0.0851, -0.0335,  0.1598, -0.0165, -0.1292, -0.0101,  0.0614],
            [ 0.0951,  0.1639,  0.1425,  0.0471, -0.0065,  0.1242,  0.1740,  0.0798,
             -0.1496,  0.1044,  0.0049, -0.0039,  0.0507,  0.1219, -0.1344, -0.0739,
              0.0153, -0.1608,  0.0176,  0.1231,  0.1180,  0.1196,  0.0942,  0.1668,
             -0.1735,  0.0610, -0.0822, -0.1125,  0.0139,  0.0715, -0.1311, -0.1524],
            [ 0.1722, -0.0933, -0.0183, -0.1403,  0.1073,  0.0237, -0.0722,  0.0828,
              0.1526, -0.0265,  0.1055,  0.0238,  0.1518, -0.0095,  0.1459, -0.0948,
             -0.0592,  0.0595, -0.0323,  0.1172, -0.0226, -0.0537, -0.0524, -0.1173,
             -0.0489, -0.1416,  0.1185,  0.1506,  0.0756, -0.0858,  0.0110, -0.0134],
            [-0.1483, -0.0189, -0.1632,  0.0639, -0.1736,  0.0263,  0.0949,  0.1423,
              0.1441, -0.1204, -0.0571, -0.0095, -0.0934, -0.0541, -0.0071,  0.0778,
              0.1376, -0.0952,  0.0384,  0.1614, -0.0972, -0.0686,  0.0631, -0.0041,
             -0.0480,  0.0125,  0.0343, -0.0615, -0.0559,  0.0343,  0.0858, -0.0640],
            [ 0.0062, -0.1718,  0.0104, -0.1249,  0.0289, -0.1222,  0.1176,  0.0595,
              0.1318,  0.1251,  0.1452,  0.1133, -0.0116,  0.0924,  0.0341,  0.0296,
             -0.1584,  0.1635,  0.1280,  0.1457,  0.1399,  0.1513,  0.1663, -0.0917,
             -0.1226, -0.0156, -0.1078, -0.1649, -0.0122,  0.0927,  0.0417,  0.0039],
            [-0.1019,  0.0150, -0.0196, -0.0307, -0.1435, -0.1224, -0.0149, -0.0350,
             -0.1457,  0.1388,  0.0321,  0.0796, -0.1161, -0.0784, -0.1638, -0.0651,
              0.1356, -0.1720,  0.0965,  0.0289,  0.0681, -0.0177, -0.1409,  0.1049,
             -0.0814,  0.0763,  0.1512,  0.1510, -0.1766, -0.1671,  0.0180, -0.1098],
            [-0.1238,  0.1767, -0.0090, -0.1724, -0.0894,  0.1031, -0.1674,  0.0348,
              0.1493,  0.1279, -0.1717,  0.1240, -0.0945, -0.0093,  0.1411,  0.0792,
              0.0471, -0.0120, -0.1017,  0.0428, -0.0915, -0.1454,  0.0993, -0.0714,
              0.0747,  0.1250,  0.1745, -0.1377,  0.0760, -0.0355, -0.1333,  0.0544],
            [-0.1462,  0.1400,  0.0823,  0.0624, -0.0666,  0.0298, -0.0386, -0.1449,
             -0.0784,  0.1730, -0.1150, -0.1504, -0.0391, -0.1244,  0.1157,  0.1280,
             -0.1670, -0.1242,  0.0244, -0.0424,  0.0276, -0.0282,  0.1302,  0.1720,
              0.0151, -0.1006, -0.0725, -0.0294,  0.0299, -0.0053,  0.0720,  0.0312],
            [ 0.1512,  0.1465, -0.0214, -0.0329, -0.1551, -0.1160, -0.1711,  0.1499,
             -0.1559, -0.1560,  0.0519,  0.0681, -0.0929, -0.1707,  0.1708,  0.0207,
              0.0358,  0.0963, -0.0357, -0.1649, -0.0066,  0.0862,  0.0841,  0.0528,
             -0.1014,  0.0213,  0.1427,  0.0799, -0.0530,  0.1383,  0.1750, -0.0412],
            [-0.0015,  0.1362, -0.1036,  0.1521,  0.0428,  0.1214, -0.0891,  0.1622,
              0.0051,  0.1049, -0.1496,  0.1162,  0.0525, -0.1709,  0.0388,  0.0997,
             -0.0933, -0.0578, -0.1116, -0.1219, -0.1161, -0.0573,  0.1610,  0.0816,
              0.1637,  0.1071,  0.1735,  0.0611, -0.1755, -0.0262,  0.1059, -0.0502],
            [-0.1590, -0.1014, -0.0301,  0.1680,  0.1743, -0.0944, -0.1628,  0.1520,
             -0.1077, -0.0240, -0.0518, -0.1610,  0.1062,  0.0675,  0.0750,  0.1533,
              0.1645,  0.0941, -0.0317,  0.1616,  0.1691, -0.1415,  0.0816, -0.1539,
              0.0551, -0.0873,  0.1093, -0.0014, -0.0646,  0.0296,  0.0099,  0.0821],
            [-0.0776,  0.0437,  0.1527,  0.0721,  0.1172, -0.1365, -0.0709, -0.1444,
             -0.0107, -0.0096,  0.0436, -0.1741,  0.0384, -0.0547,  0.0973, -0.1340,
             -0.1163, -0.1587,  0.0155, -0.0050, -0.1426, -0.1644, -0.0285,  0.1734,
              0.0310,  0.1168,  0.0195, -0.0133, -0.1557, -0.0277, -0.1099, -0.1665],
            [-0.1351,  0.0648, -0.1566,  0.1130, -0.1280, -0.1355,  0.1487,  0.1617,
              0.0101,  0.1749,  0.0658, -0.0783, -0.0760,  0.0182, -0.1726, -0.0524,
             -0.0842,  0.0378, -0.0885, -0.1729, -0.1124,  0.1251, -0.0356, -0.1331,
             -0.1463,  0.0752,  0.0150, -0.1492, -0.1112, -0.1569, -0.0942, -0.1177],
            [ 0.1064,  0.1641, -0.0470, -0.1650, -0.0817, -0.0256,  0.0727,  0.0006,
             -0.0386,  0.1136,  0.0664, -0.0881, -0.1431,  0.0073,  0.1605, -0.1007,
              0.0932, -0.1090,  0.1740, -0.1245, -0.1634, -0.0978,  0.0419, -0.1756,
             -0.1634, -0.0036, -0.1719, -0.1329,  0.1040, -0.1101, -0.0228, -0.0696],
            [ 0.0607, -0.1152,  0.0515, -0.1260,  0.0946,  0.0849, -0.0166,  0.0661,
             -0.1080,  0.1381,  0.0205, -0.1644,  0.1621,  0.1494, -0.0393, -0.1761,
              0.1229, -0.0595, -0.0349,  0.0861, -0.0183,  0.1491, -0.0850, -0.1251,
             -0.1227, -0.0830,  0.1248, -0.1408,  0.0043,  0.1699, -0.0942, -0.0666],
            [-0.1370, -0.1550, -0.0089,  0.0830,  0.1532,  0.1474, -0.0253, -0.1312,
              0.0644, -0.0508,  0.0944, -0.1388, -0.1637, -0.0980, -0.0410,  0.0174,
             -0.0032,  0.0544, -0.1290, -0.0948,  0.1059,  0.0469, -0.1130,  0.0791,
              0.0410, -0.0457,  0.0691, -0.1226,  0.0160, -0.1255, -0.0521,  0.0237]],
           requires_grad=True)
    Parameter name: bias, shape:torch.Size([16]), Parameter: Parameter containing:
    tensor([-0.0587, -0.1316, -0.1294,  0.0608,  0.0629, -0.0954, -0.0122, -0.0232,
            -0.0588, -0.0454, -0.0374,  0.1164,  0.1096, -0.1144, -0.1118, -0.0008],
           requires_grad=True)


### 练习 5.2.2

查看初始化模块文档以了解不同的初始化方法。

**解答：**

&emsp;&emsp;通过查看深度学习框架文档，有以下初始化方法 （参考链接：https://pytorch.org/docs/stable/nn.init.html ）
- `torch.nn.init.uniform_(tensor, a=0.0, b=1.0)`：从均匀分布$U(a,b)$中提取填充输入张量。

- `torch.nn.init.normal_(tensor, mean=0.0, std=1.0)`：从正态分布$N(mean, std^2)$中提取填充输入张量。

- `torch.nn.init.constant_(tensor, val)`：以一确定数值初始化张量。

- `torch.nn.init.ones_(tensor)`：用标量值 1 填充输入张量。

- `torch.nn.init.zeros_(tensor)`：用标量值 0 填充输入张量。

- `torch.nn.init.eye_(tensor)`：用单位矩阵填充二维输入张量。

- `torch.nn.init.xavier_uniform_(tensor, gain=1.0)`：从均匀分布$U(−a, a)$中采样，初始化输入张量，其中$a$的值由如下公式确定

  $$a= gain * \sqrt{\frac{6}{fan_{in}+fan_{out}}}$$
  
  其中$gain$的取值如下表所示
  
非线性函数 | gain值
---|---
Linear/Identity | 1
Conv1D | 1
Conv2D | 1
Conv3D | 1
Sigmoid | 1
Tanh | $\displaystyle\frac{5}{3}$
ReLU | $\sqrt{2}$
Leaky ReLU | $$\sqrt{\frac{2}{1+negative\_slope^2}}$$
SELU | 1 (adaptive)

- `torch.nn.init.xavier_normal_(tensor, gain=1.0)`:从正态分布$N(0,std^2)$中采样，初始化输入张量，其中std值由下式确定：

  $$a= gain * \sqrt{\frac{2}{fan_{in}+fan_{out}}}$$

- `torch.nn.init.kaiming_uniform_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')`:服从均匀分布$U(−bound, bound)$，其中$bound$值由下式确定

  $$bound= gain * \sqrt{\frac{3}{fan_{mode}}}$$

- `torch.nn.init.kaiming_normal_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')`:服从从正态分布$N(0,std^2)$中采样，其中$std$值由下式确定

  $$std= \frac{gain}{\sqrt{fan_{mode}}}$$
  
- `torch.nn.init.trunc_normal_(tensor, mean=0.0, std=1.0, a=- 2.0, b=2.0)`:用从截断的正态分布中提取的值填充输入张量。这些值实际上是从正态分布 $N(mean, std^2)$中提取的。

- `torch.nn.init.sparse_(tensor, sparsity, std=0.01)`：将 2D 输入张量填充为稀疏矩阵，其中非零元素将从正态分布$N(0,0.01)$中提取。

### 练习 5.2.3

构建包含共享参数层的多层感知机并对其进行训练。在训练过程中，观察模型各层的参数和梯度。

**解答：** 

&emsp;&emsp;在训练过程中，我们每个epoch都打印了每层的参数和梯度。可以看到shared_fc层的参数和梯度都是相同的，因为它们共享同一个参数。


```python
import torch
import torch.nn as nn
import torch.optim as optim

# 模型参数
input_size = 4
hidden_size = 8
output_size = 4
lr = 0.01
epochs = 2

# 构建带有共享参数层的多层感知机
shared_fc = nn.Linear(hidden_size, hidden_size)
MLP = nn.Sequential(nn.Linear(input_size, hidden_size), nn.ReLU(),
                    shared_fc, nn.ReLU(),
                    shared_fc, nn.ReLU(),
                    nn.Linear(hidden_size, output_size)
)

# 训练数据
X = torch.randn(1, input_size)
Y = torch.randn(1, output_size)
# 优化器
optimizer = optim.SGD(MLP.parameters(), lr=lr)
# 训练模型
for epoch in range(epochs):
    # 前向传播和计算损失
    Y_pred = MLP(X)
    loss = nn.functional.mse_loss(Y_pred, Y)
    # 反向传播和更新梯度
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    # 打印每层的参数和梯度
    for name, param in MLP.named_parameters():
        print(name, param.data, param.grad)
    print('Epoch: {}, Loss: {}'.format(epoch, loss.item()))
```

    0.weight tensor([[-0.1223, -0.2865, -0.1142, -0.1206],
            [ 0.0428,  0.0664, -0.3060, -0.2732],
            [-0.2939,  0.3809,  0.4409,  0.2039],
            [-0.2752,  0.1679,  0.3662,  0.4009],
            [ 0.3634, -0.1864,  0.2473,  0.2193],
            [ 0.2975, -0.3068,  0.3993, -0.3316],
            [-0.1651, -0.0725,  0.1861,  0.4409],
            [ 0.4155,  0.1977,  0.2613, -0.2014]]) tensor([[ 0.0000,  0.0000, -0.0000,  0.0000],
            [-0.2118, -0.0760,  0.1077, -0.0336],
            [ 0.0000,  0.0000, -0.0000,  0.0000],
            [ 0.0000,  0.0000, -0.0000,  0.0000],
            [-0.1781, -0.0639,  0.0906, -0.0283],
            [ 0.0000,  0.0000, -0.0000,  0.0000],
            [ 0.0000,  0.0000, -0.0000,  0.0000],
            [-0.1566, -0.0562,  0.0796, -0.0249]])
    0.bias tensor([ 0.3000, -0.1717,  0.4032, -0.0697, -0.2140, -0.3950, -0.2322, -0.2666]) tensor([ 0.0000, -0.0968,  0.0000,  0.0000, -0.0814,  0.0000,  0.0000, -0.0716])
    2.weight tensor([[-0.2533, -0.2829, -0.2677,  0.1634, -0.1019,  0.1053, -0.1058,  0.2976],
            [ 0.2634, -0.0198, -0.1916,  0.0107, -0.1541, -0.2566,  0.0304,  0.3258],
            [-0.2977, -0.1825,  0.1853, -0.2739, -0.2335,  0.1091, -0.0049, -0.2277],
            [-0.2091, -0.3297, -0.1003,  0.0363, -0.3410,  0.2284,  0.1777, -0.1423],
            [-0.0588, -0.0658, -0.1319, -0.2297,  0.3178, -0.2209, -0.3093, -0.2859],
            [-0.0958, -0.1032,  0.3259, -0.0770, -0.0698, -0.2427,  0.2445, -0.2620],
            [ 0.3108, -0.1900,  0.3295,  0.3409,  0.3048,  0.2717, -0.2628,  0.0300],
            [-0.2384, -0.3000,  0.2284, -0.3389, -0.2287,  0.0040,  0.1502, -0.0355]]) tensor([[ 0.0133,  0.0644,  0.0000,  0.0119,  0.0738,  0.0000,  0.0034,  0.0991],
            [ 0.0303,  0.0297,  0.0000,  0.0272,  0.0414,  0.0000,  0.0078, -0.0126],
            [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
            [ 0.0163,  0.0722,  0.0000,  0.0146,  0.0832,  0.0000,  0.0042,  0.1078],
            [ 0.0210,  0.0708,  0.0000,  0.0188,  0.0831,  0.0000,  0.0054,  0.0936],
            [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
            [ 0.0524,  0.0761,  0.0000,  0.0469,  0.0984,  0.0000,  0.0134,  0.0289],
            [-0.0084, -0.0036,  0.0000, -0.0075, -0.0065,  0.0000, -0.0022,  0.0128]])
    2.bias tensor([ 0.0603,  0.1333, -0.2945,  0.3031,  0.3488, -0.0981, -0.0183,  0.3488]) tensor([ 0.2903,  0.1092,  0.0000,  0.3240,  0.3125,  0.0000,  0.3057, -0.0086])
    6.weight tensor([[-0.3528, -0.2028, -0.1961,  0.3527, -0.1456,  0.0696,  0.3069,  0.1214],
            [ 0.1391,  0.0669,  0.0042,  0.2146, -0.2063, -0.1007,  0.2317, -0.0355],
            [ 0.2470, -0.0490,  0.0801,  0.2788, -0.3073, -0.1294, -0.1830, -0.2430],
            [ 0.1278,  0.2530,  0.0247, -0.0373,  0.2861,  0.2157,  0.2341, -0.0928]]) tensor([[ 0.0043,  0.0450,  0.0000,  0.0207,  0.0793,  0.0000,  0.0228,  0.0360],
            [ 0.0102,  0.1064,  0.0000,  0.0489,  0.1874,  0.0000,  0.0540,  0.0851],
            [-0.0011, -0.0117, -0.0000, -0.0054, -0.0206, -0.0000, -0.0059, -0.0093],
            [ 0.0200,  0.2093,  0.0000,  0.0961,  0.3685,  0.0000,  0.1062,  0.1674]])
    6.bias tensor([-0.1153,  0.0429,  0.1363, -0.0886]) tensor([ 0.2409,  0.5692, -0.0625,  1.1196])
    Epoch: 0, Loss: 1.639384150505066
    0.weight tensor([[-0.1223, -0.2865, -0.1142, -0.1206],
            [ 0.0449,  0.0672, -0.3071, -0.2729],
            [-0.2939,  0.3809,  0.4409,  0.2039],
            [-0.2752,  0.1679,  0.3662,  0.4009],
            [ 0.3652, -0.1857,  0.2464,  0.2196],
            [ 0.2975, -0.3068,  0.3993, -0.3316],
            [-0.1651, -0.0725,  0.1861,  0.4409],
            [ 0.4171,  0.1983,  0.2605, -0.2012]]) tensor([[ 0.0000,  0.0000,  0.0000,  0.0000],
            [-0.2112, -0.0758,  0.1074, -0.0335],
            [ 0.0000,  0.0000,  0.0000,  0.0000],
            [ 0.0000,  0.0000,  0.0000,  0.0000],
            [-0.1782, -0.0639,  0.0906, -0.0283],
            [ 0.0000,  0.0000,  0.0000,  0.0000],
            [ 0.0000,  0.0000,  0.0000,  0.0000],
            [-0.1550, -0.0556,  0.0788, -0.0246]])
    0.bias tensor([ 0.3000, -0.1707,  0.4032, -0.0697, -0.2131, -0.3950, -0.2322, -0.2659]) tensor([ 0.0000, -0.0965,  0.0000,  0.0000, -0.0815,  0.0000,  0.0000, -0.0708])
    2.weight tensor([[-0.2534, -0.2835, -0.2677,  0.1633, -0.1027,  0.1053, -0.1058,  0.2966],
            [ 0.2631, -0.0201, -0.1916,  0.0105, -0.1545, -0.2566,  0.0303,  0.3260],
            [-0.2977, -0.1825,  0.1853, -0.2739, -0.2335,  0.1091, -0.0049, -0.2277],
            [-0.2093, -0.3304, -0.1003,  0.0361, -0.3418,  0.2284,  0.1777, -0.1433],
            [-0.0590, -0.0665, -0.1319, -0.2298,  0.3170, -0.2209, -0.3093, -0.2868],
            [-0.0958, -0.1032,  0.3259, -0.0770, -0.0698, -0.2427,  0.2445, -0.2620],
            [ 0.3103, -0.1908,  0.3295,  0.3405,  0.3039,  0.2717, -0.2629,  0.0298],
            [-0.2383, -0.3000,  0.2284, -0.3388, -0.2286,  0.0040,  0.1502, -0.0356]]) tensor([[ 0.0126,  0.0649,  0.0000,  0.0106,  0.0736,  0.0000,  0.0030,  0.0985],
            [ 0.0285,  0.0276,  0.0000,  0.0241,  0.0384,  0.0000,  0.0068, -0.0146],
            [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
            [ 0.0153,  0.0727,  0.0000,  0.0129,  0.0828,  0.0000,  0.0037,  0.1074],
            [ 0.0194,  0.0695,  0.0000,  0.0164,  0.0806,  0.0000,  0.0047,  0.0911],
            [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
            [ 0.0496,  0.0739,  0.0000,  0.0419,  0.0945,  0.0000,  0.0119,  0.0261],
            [-0.0082, -0.0036,  0.0000, -0.0069, -0.0064,  0.0000, -0.0020,  0.0129]])
    2.bias tensor([ 0.0574,  0.1323, -0.2945,  0.2998,  0.3458, -0.0981, -0.0213,  0.3489]) tensor([ 0.2871,  0.1065,  0.0000,  0.3208,  0.3036,  0.0000,  0.3030, -0.0108])
    6.weight tensor([[-0.3529, -0.2033, -0.1961,  0.3525, -0.1464,  0.0696,  0.3067,  0.1211],
            [ 0.1391,  0.0659,  0.0042,  0.2141, -0.2081, -0.1007,  0.2312, -0.0364],
            [ 0.2470, -0.0489,  0.0801,  0.2788, -0.3071, -0.1294, -0.1829, -0.2430],
            [ 0.1276,  0.2510,  0.0247, -0.0382,  0.2825,  0.2157,  0.2332, -0.0945]]) tensor([[ 0.0033,  0.0439,  0.0000,  0.0201,  0.0786,  0.0000,  0.0206,  0.0370],
            [ 0.0077,  0.1036,  0.0000,  0.0474,  0.1854,  0.0000,  0.0485,  0.0873],
            [-0.0009, -0.0115,  0.0000, -0.0053, -0.0205,  0.0000, -0.0054, -0.0097],
            [ 0.0152,  0.2039,  0.0000,  0.0934,  0.3651,  0.0000,  0.0955,  0.1719]])
    6.bias tensor([-0.1177,  0.0373,  0.1369, -0.0997]) tensor([ 0.2392,  0.5642, -0.0625,  1.1109])
    Epoch: 1, Loss: 1.613648772239685


### 练习 5.2.4

为什么共享参数是个好主意？

**解答：** 

&emsp;&emsp;1. 节约内存：共享参数可以减少模型中需要存储的参数数量，从而减少内存占用。

&emsp;&emsp;2. 加速收敛：共享参数可以让模型更加稳定，加速收敛。

&emsp;&emsp;3. 提高泛化能力：共享参数可以帮助模型更好地捕捉数据中的共性，提高模型的泛化能力。

&emsp;&emsp;4. 加强模型的可解释性：共享参数可以让模型更加简洁明了，加强模型的可解释性。 

## 5.3 延后初始化 

### 练习 5.3.1 

如果指定了第一层的输入尺寸，但没有指定后续层的尺寸，会发生什么？是否立即进行初始化？

**解答：** 

&emsp;&emsp;可以正常运行。第一层会立即初始化,但其他层是直到数据第一次通过模型传递才会初始化。


```python
import torch
from torch import nn

"""延后初始化"""
net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))
# 尚未初始化
print(net)

X = torch.rand(2, 20)
net(X)
print(net)
```

    Sequential(
      (0): LazyLinear(in_features=0, out_features=256, bias=True)
      (1): ReLU()
      (2): LazyLinear(in_features=0, out_features=10, bias=True)
    )
    Sequential(
      (0): Linear(in_features=20, out_features=256, bias=True)
      (1): ReLU()
      (2): Linear(in_features=256, out_features=10, bias=True)
    )


### 练习 5.3.2

如果指定了不匹配的维度会发生什么？

**解答：** 

&emsp;&emsp;会由于矩阵乘法的维度不匹配而报错。在下面的代码中便指定了不匹配的维度。

&emsp;&emsp;由于第一层 nn.Linear(20, 256) 的输入维度为 20，所以输入数据 X 的最后一维必须为 20 才能与该层的权重矩阵相乘。


```python
import torch

net = nn.Sequential(
    nn.Linear(20, 256), nn.ReLU(),
    nn.LazyLinear(128), nn.ReLU(),
    nn.LazyLinear(10))

X = torch.rand(2, 10)
try:
    net(X)
except Exception as e:
    print(e)
```

    mat1 and mat2 shapes cannot be multiplied (2x10 and 20x256)


### 练习 5.3.3 

如果输入具有不同的维度，需要做什么？提示：查看参数绑定的相关内容。

**解答：**

&emsp;&emsp;添加一个额外的线性层，并将第一个线性层的权重与该层的权重绑定在一起。这样就可以解决维度不匹配的问题，并且保持模型的权重不变。注意，在上面的代码中，我们假设第一个线性层的偏置项为零，因此不需要对其进行参数绑定。


```python
import torch
import torch.nn as nn

net = nn.Sequential(
    nn.Linear(20, 256), nn.ReLU(),
    nn.Linear(256, 128), nn.ReLU(),
    nn.Linear(128, 10))

# 添加额外的线性层
extra_layer = nn.Linear(10, 256)

# 将第一个线性层与额外的线性层的权重进行绑定
net[0].weight = extra_layer.weight

# 使用新的输入（维度为20）调用模型
X = torch.rand(2, 10)
net(X)
```




    tensor([[-0.0332, -0.2036,  0.0035,  0.0578, -0.0328,  0.0436, -0.1417,  0.0474,
              0.1288,  0.0482],
            [-0.0327, -0.1565,  0.0213,  0.0434, -0.0187,  0.0178, -0.1491, -0.0147,
              0.0906,  0.0088]], grad_fn=<AddmmBackward0>)



## 5.4 自定义层 

### 练习 5.4.1 

设计一个接受输入并计算张量降维的层，它返回$y_k = \sum_{i, j} W_{ijk} x_i x_j$。

**解答：** 

&emsp;&emsp;这个公式表示一个线性变换，将输入张量$x$中所有可能的二元组$(x_i,x_j)$进行组合，并对它们进行加权求和。其中，$W_{ijk}$表示权重张量中第$i,j,k$个元素的值。具体而言，该公式计算了输入张量$x$中所有二元组$(x_i, x_j)$对应的特征向量$u_{ij}$为：


$$
u_{ij} = x_i \cdot x_j
$$


&emsp;&emsp;然后，根据权重张量$W$中的权重$W_{ijk}$，对所有特征向量$u_{ij}$进行线性组合，得到输出向量$y_k$为：


$$
y_k = \sum_{i,j} W_{ijk} u_{ij} = \sum_{i,j} W_{ijk} x_i x_j
$$


&emsp;&emsp;该操作可以被视为一种降维操作，将高维输入$x$映射到低维输出空间$y$中。


```python
class TensorReduction(nn.Module):
    def __init__(self, dim1, dim2):
        super(TensorReduction, self).__init__()
        # 定义一个可训练的权重参数，维度为(dim2, dim1, dim1)
        self.weight = nn.Parameter(torch.rand(dim2, dim1, dim1))

    def forward(self, X):
        # 初始化一个全零张量，大小为(X.shape[0], self.weight.shape[0])
        Y = torch.zeros(X.shape[0], self.weight.shape[0])
        for k in range(self.weight.shape[0]):
            # 计算temp = X @ weight[k] @ X^T
            temp = torch.matmul(X, self.weight[k]) @ X.T
            # 取temp的对角线元素，存入Y[:, k]
            Y[:, k] = temp.diagonal()
        return Y

# 创建一个TensorReduction层，dim1=10, dim2=5
layer = TensorReduction(10, 5)
# 创建一个大小为(2, 10)的张量X
X = torch.rand(2, 10)
# 对layer(X)进行前向传播，返回一个大小为(2, 5)的张量
layer(X).shape
```




    torch.Size([2, 5])



### 练习 5.4.2 

设计一个返回输入数据的傅立叶系数前半部分的层。

**解答：** 

&emsp;&emsp;根据[维基百科](https://en.wikipedia.org/wiki/Fourier_series)中:
> &emsp;&emsp;傅里叶级数是将任意周期函数表示为一组正弦和余弦函数的无限级数的方法。假设$f(x)$是在区间$[-L,L]$中定义的一个函数，其周期为$2L$，则其傅里叶级数可表示为：
> 
> $$f(x) = \frac{a_0}{2} + \sum_{n=1}^{\infty} [a_n \cos(\frac{n\pi x}{L}) + b_n \sin(\frac{n\pi x}{L})]$$
> 
> &emsp;&emsp;其中，系数$a_0, a_n$和$b_n$可以通过以下公式计算得出：
> 
> $$a_0 = \frac{1}{2L} \int_{-L}^{L} f(x)dx$$
> 
> $$a_n = \frac{1}{L} \int_{-L}^{L} f(x) \cos(\frac{n\pi x}{L}) dx, n>0$$
> 
> $$b_n = \frac{1}{L} \int_{-L}^{L} f(x) \sin(\frac{n\pi x}{L}) dx, n>0$$
> 
> &emsp;&emsp;系数$a_n$和$b_n$实际上是$f(x)$与$\cos(\frac{n\pi x}{L})$和$\sin(\frac{n\pi x}{L})$的内积，而$a_0$是$f(x)$平均值的一半。

&emsp;&emsp;在torch中有相应的函数可以轻松的实现傅里叶级数，如下代码所示：


```python
import torch.nn as nn
import torch.fft as fft

class FourierLayer(nn.Module):
    def __init__(self):
        super(FourierLayer, self).__init__()

    def forward(self, x):
        # 对输入的张量 x 进行快速傅里叶变换
        x = fft.fftn(x)
        # 取出第三个维度的前半部分，即去掉直流分量和镜像分量
        x = x[:, :, :x.shape[2] // 2]
        # 返回处理后的张量
        return x

# 创建一个随机数值为 [0, 1) 的形状为 (1, 2, 5) 的张量 X
X = torch.rand(1, 2, 5)
# 实例化一个 FourierLayer 的网络对象 net
net = FourierLayer()
# 将 X 输入到网络 net 中进行前向计算，并输出结果
net(X)
```




    tensor([[[ 5.7054+0.0000j,  0.0402+0.0496j],
             [-0.2460+0.0000j,  1.0908-0.0832j]]])



## 5.5 读写文件 

###  练习 5.5.1

即使不需要将经过训练的模型部署到不同的设备上，存储模型参数还有什么实际的好处？

**解答：** 

&emsp;&emsp;1. 加速模型训练：存储模型参数可以避免每次重新训练模型时需要重复计算之前已经计算过的权重和偏置。

&emsp;&emsp;2. 节省内存空间：保存模型参数比保存完整的模型文件更加节省内存空间，这在处理大型模型或使用内存受限设备时尤为重要。

&emsp;&emsp;3. 便于共享和复现：存储模型参数可以方便地共享和复现已经训练好的模型，其他人可以直接加载这些参数并使用它们进行预测或微调。

&emsp;&emsp;4. 便于调试和分析：通过检查模型参数，可以更容易地诊断模型中存在的问题，并对其进行调整和优化。

### 练习 5.5.2

假设我们只想复用网络的一部分，以将其合并到不同的网络架构中。比如想在一个新的网络中使用之前网络的前两层，该怎么做？

**解答：** 

&emsp;&emsp;使用保存模型某层参数的办法，保存网络的前两层，然后再加载到新的网络中使用。


```python
import torch
from torch import nn
from torch.nn import functional as F

class MLP(nn.Module):             # 定义 MLP 类
    def __init__(self):
        super().__init__()
        self.hidden = nn.Linear(20, 256)   # 定义隐藏层层，输入尺寸为 20，输出尺寸为 256
        self.output = nn.Linear(256, 10)   # 定义输出层，输入尺寸为 256，输出尺寸为 10

    def forward(self, x):          # 定义前向传播函数
        return self.output(F.relu(self.hidden(x)))  # 使用 ReLU 激活函数，计算隐藏层和输出层的输出

class MLP_new(nn.Module):             # 定义 MLP 类
    def __init__(self):
        super().__init__()
        self.hidden = nn.Linear(20, 256)   # 定义隐藏层层，输入尺寸为 20，输出尺寸为 256
        self.output = nn.Linear(256, 10)   # 定义输出层，输入尺寸为 256，输出尺寸为 10

    def forward(self, x):          # 定义前向传播函数
        return self.output(F.relu(self.hidden(x)))  # 使用 ReLU 激活函数，计算隐藏层和输出层的输出

net = MLP()                       # 创建 MLP 的实例
torch.save(net.hidden.state_dict(), 'mlp.hidden.params')  # 将隐藏层的参数保存到文件中
clone = MLP_new()                     # 创建另一个 MLP 的实例
clone.hidden.load_state_dict(torch.load('mlp.hidden.params'))  # 加载已保存的参数到克隆实例的隐藏层中
print(clone.hidden.weight == net.hidden.weight)  # 比较两个 MLP 实例的隐藏层权重是否相等，并输出结果
```

    tensor([[True, True, True,  ..., True, True, True],
            [True, True, True,  ..., True, True, True],
            [True, True, True,  ..., True, True, True],
            ...,
            [True, True, True,  ..., True, True, True],
            [True, True, True,  ..., True, True, True],
            [True, True, True,  ..., True, True, True]])


### 练习 5.5.3

如何同时保存网络架构和参数？需要对架构加上什么限制？

**解答：** 

&emsp;&emsp;在PyTorch中，可以使用`torch.save()`函数同时保存网络架构和参数。为了保存网络架构，需要将模型的结构定义在一个Python类中，并将该类实例化为模型对象。此外，必须确保该类的构造函数不包含任何随机性质的操作，例如dropout层的随机丢弃率应该是固定的。


```python
import torch
from torch import nn
from torch.nn import functional as F

class MLP(nn.Module):             # 定义 MLP 类
    def __init__(self):
        super().__init__()
        self.hidden = nn.Linear(20, 256)   # 定义隐藏层层，输入尺寸为 20，输出尺寸为 256
        self.output = nn.Linear(256, 10)   # 定义输出层，输入尺寸为 256，输出尺寸为 10

    def forward(self, x):          # 定义前向传播函数
        return self.output(F.relu(self.hidden(x)))  # 使用 ReLU 激活函数，计算隐藏层和输出层的输出

net = MLP()

# 存储模型
torch.save(net.state_dict(), 'model.pt')

# 导入模型
model = torch.load('model.pt')
model
```




    OrderedDict([('hidden.weight',
                  tensor([[-0.1156,  0.1863, -0.0175,  ..., -0.1466, -0.2019,  0.0216],
                          [ 0.0895, -0.1677, -0.0554,  ..., -0.1252, -0.2170,  0.0083],
                          [ 0.0219,  0.1506, -0.0535,  ...,  0.1031, -0.1729,  0.1682],
                          ...,
                          [-0.1097,  0.0599, -0.1763,  ...,  0.1636,  0.1202, -0.0097],
                          [-0.1457,  0.2051,  0.2128,  ...,  0.1272, -0.0476, -0.0765],
                          [-0.1741, -0.1148,  0.1382,  ...,  0.0671,  0.1791, -0.1454]])),
                 ('hidden.bias',
                  tensor([ 0.1766, -0.0295, -0.1861, -0.0068,  0.2015, -0.0649, -0.0570, -0.2172,
                           0.0714, -0.1939,  0.2156,  0.0916,  0.1790, -0.1583,  0.0692, -0.1513,
                          -0.1918,  0.1775,  0.2189, -0.0194, -0.1804, -0.1995, -0.0825, -0.0024,
                           0.0511,  0.1024,  0.0159,  0.1635,  0.1716,  0.2139,  0.1466, -0.1488,
                           0.0122, -0.0310,  0.1765, -0.1931,  0.1383,  0.0725,  0.1630,  0.0622,
                          -0.0836,  0.0341, -0.0112,  0.0356,  0.0274, -0.1440,  0.0020, -0.0793,
                          -0.1728,  0.1986, -0.2185,  0.0159, -0.1773, -0.1332,  0.0944, -0.0076,
                           0.1248,  0.0570, -0.1019, -0.1706, -0.1054,  0.1165,  0.1299, -0.0062,
                          -0.1740,  0.1895, -0.1824, -0.1933,  0.1936, -0.1204,  0.2207,  0.1267,
                           0.1236,  0.0671, -0.1981,  0.0835, -0.0292,  0.0989,  0.0729,  0.1339,
                           0.0196, -0.0299, -0.1500,  0.0006,  0.1628, -0.0434,  0.0067, -0.1456,
                           0.1304, -0.1666,  0.1803,  0.2052,  0.1699, -0.1716,  0.1595,  0.1105,
                          -0.1859,  0.0438,  0.0998,  0.0628, -0.1161,  0.0794, -0.1899,  0.1666,
                          -0.1218, -0.1331, -0.0222, -0.1483,  0.2037,  0.0646,  0.0780, -0.1663,
                          -0.0258, -0.0954,  0.0507,  0.1409,  0.0035,  0.0075,  0.0272,  0.1016,
                           0.2113, -0.0838,  0.2073, -0.0815,  0.1675,  0.0785, -0.0213,  0.0250,
                           0.0124, -0.1796,  0.1304,  0.0415,  0.0501, -0.0830, -0.1177, -0.0184,
                           0.1451,  0.2138, -0.0849,  0.1035, -0.1483,  0.1134,  0.0372,  0.0114,
                           0.2180,  0.1472, -0.0085,  0.0218,  0.0432,  0.0168, -0.0995,  0.0535,
                           0.2126,  0.0948,  0.0413, -0.1711, -0.0574,  0.1348, -0.1017,  0.2058,
                           0.0341, -0.0094,  0.0182, -0.1423,  0.0180,  0.0717,  0.0249,  0.1916,
                           0.0109, -0.0301, -0.0721,  0.0041, -0.1025, -0.1381, -0.1428, -0.2062,
                           0.1903,  0.2106,  0.0542, -0.0143, -0.0606, -0.0076,  0.1080,  0.0385,
                          -0.1319, -0.0086,  0.1028, -0.2122, -0.1795, -0.0077, -0.2020, -0.0379,
                           0.1401,  0.0745, -0.2204, -0.2118, -0.2111, -0.1001, -0.1703,  0.0028,
                          -0.1287, -0.1110, -0.1749,  0.0086,  0.1138, -0.0624,  0.0978,  0.1115,
                          -0.0148,  0.0990,  0.1336,  0.0328,  0.0651,  0.1320,  0.1443,  0.0800,
                           0.2155,  0.1570,  0.0192, -0.1268,  0.1085,  0.0400, -0.0108,  0.1133,
                          -0.1684,  0.2001, -0.0513, -0.0052, -0.0617,  0.1917,  0.1170,  0.1218,
                          -0.2125, -0.0716, -0.0619, -0.0967, -0.0929, -0.2126,  0.2150,  0.0424,
                          -0.2172,  0.1137,  0.0601, -0.0266,  0.1998,  0.0429, -0.0619, -0.0983,
                          -0.1875,  0.0654, -0.1324, -0.0647, -0.1327, -0.0441, -0.0721,  0.0988])),
                 ('output.weight',
                  tensor([[-0.0009,  0.0294, -0.0051,  ..., -0.0006,  0.0582,  0.0326],
                          [ 0.0174, -0.0336, -0.0176,  ..., -0.0027,  0.0142, -0.0435],
                          [-0.0136,  0.0579, -0.0130,  ..., -0.0463,  0.0178, -0.0148],
                          ...,
                          [ 0.0538,  0.0350, -0.0365,  ..., -0.0312,  0.0151, -0.0308],
                          [ 0.0241, -0.0240, -0.0613,  ...,  0.0461, -0.0597,  0.0160],
                          [ 0.0439,  0.0540,  0.0307,  ..., -0.0360, -0.0272, -0.0426]])),
                 ('output.bias',
                  tensor([-0.0548, -0.0609, -0.0241,  0.0394, -0.0228, -0.0331, -0.0540, -0.0416,
                           0.0147,  0.0345]))])



## 5.6 GPU 

### 练习 5.6.1 

尝试一个计算量更大的任务，比如大矩阵的乘法，看看CPU和GPU之间的速度差异。再试一个计算量很小的任务呢？

**解答：** 

&emsp;&emsp;计算量很大的任务：使用GPU速度明显更快

&emsp;&emsp;计算量很小的任务：CPU速度可能更快，因为数据传输到GPU需要时间


```python
import time
import torch

# 计算量较大的任务
X = torch.rand((10000, 10000))
Y = X.cuda(0)
time_start = time.time()
Z = torch.mm(X, X)
time_end = time.time()
print(f'cpu time cost: {round((time_end - time_start) * 1000, 2)}ms')
time_start = time.time()
Z = torch.mm(Y, Y)
time_end = time.time()
print(f'gpu time cost: {round((time_end - time_start) * 1000, 2)}ms')

# 计算量很小的任务
X = torch.rand((100, 100))
Y = X.cuda(0)
time_start = time.time()
Z = torch.mm(X, X)
time_end = time.time()
print(f'cpu time cost: {round((time_end - time_start) * 1000)}ms')
time_start = time.time()
Z = torch.mm(Y, Y)
time_end = time.time()
print(f'gpu time cost: {round((time_end - time_start) * 1000)}ms')
```

    cpu time cost: 10666.56ms
    gpu time cost: 1193.48ms
    cpu time cost: 0ms
    gpu time cost: 0ms


### 练习 5.6.2

我们应该如何在GPU上读写模型参数？

**解答：** 

&emsp;&emsp;使用`net.to(device)`将模型迁移到GPU上，然后再按照之前的方法读写参数。


```python
import torch
from torch import nn
from torch.nn import functional as F

class MLP(nn.Module):             # 定义 MLP 类
    def __init__(self):
        super().__init__()
        self.hidden = nn.Linear(20, 256)   # 定义隐藏层层，输入尺寸为 20，输出尺寸为 256
        self.output = nn.Linear(256, 10)   # 定义输出层，输入尺寸为 256，输出尺寸为 10

    def forward(self, x):          # 定义前向传播函数
        return self.output(F.relu(self.hidden(x)))  # 使用 ReLU 激活函数，计算隐藏层和输出层的输出

# 选择GPU，没有GPU就选CPU
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
# 创建模型实例对象
net = MLP()
# 将模型参数传输到GPU上
net.to(device)
# 访问模型参数
net.state_dict()
```




    OrderedDict([('hidden.weight',
                  tensor([[-0.0432, -0.1930,  0.2123,  ...,  0.0261,  0.1068, -0.1317],
                          [ 0.0600, -0.0118,  0.1694,  ...,  0.0250,  0.1431, -0.0099],
                          [-0.0433,  0.0347,  0.0848,  ..., -0.1433, -0.1530,  0.0941],
                          ...,
                          [ 0.0463, -0.0126, -0.1281,  ...,  0.0086,  0.1511, -0.0612],
                          [-0.0961,  0.0380,  0.1699,  ...,  0.0807, -0.1931, -0.1482],
                          [ 0.0692,  0.1754, -0.1558,  ..., -0.0194, -0.0038, -0.1192]],
                         device='cuda:0')),
                 ('hidden.bias',
                  tensor([-0.0917,  0.1111,  0.0606,  0.1126, -0.0134,  0.1238, -0.1166, -0.1253,
                          -0.2017, -0.0720, -0.1909, -0.1292, -0.2150, -0.0914,  0.0252,  0.1119,
                          -0.1487,  0.1433, -0.0134, -0.0190, -0.0965,  0.0042, -0.0422, -0.1297,
                          -0.1726,  0.1291,  0.0240, -0.0975, -0.0563, -0.0059,  0.0663, -0.1109,
                           0.1318, -0.0842,  0.1192, -0.0094, -0.0738,  0.0525, -0.0794,  0.0294,
                          -0.0594, -0.0539,  0.0464, -0.1584, -0.1031,  0.0378, -0.0634, -0.0937,
                           0.1925,  0.0734, -0.0786,  0.1569,  0.2214,  0.0881, -0.0914, -0.0423,
                          -0.1731,  0.0549,  0.1029, -0.1754,  0.0755,  0.1749,  0.1429, -0.1584,
                           0.0883, -0.0443,  0.2183,  0.0243, -0.1041, -0.1404, -0.2173,  0.0926,
                          -0.0041, -0.1878,  0.0221,  0.0115,  0.0226, -0.2167,  0.2117, -0.0912,
                           0.1352, -0.0309, -0.1846,  0.0896, -0.0330, -0.1251, -0.0828,  0.1357,
                          -0.1486,  0.1482,  0.2110, -0.1634,  0.0424, -0.0310, -0.2083,  0.1316,
                           0.1220,  0.1329, -0.0534,  0.1006, -0.0690, -0.2045,  0.1394, -0.0197,
                          -0.1316,  0.0192, -0.1713, -0.0355,  0.0741, -0.1259, -0.0676,  0.1689,
                           0.0559,  0.0930,  0.1198,  0.1338, -0.0996, -0.1344, -0.1331, -0.1800,
                           0.1331,  0.1098,  0.0771, -0.1508, -0.0768, -0.1873,  0.0574,  0.1658,
                           0.0138,  0.0859,  0.1685,  0.0780, -0.0465, -0.0818,  0.0875,  0.1189,
                          -0.0610, -0.2144,  0.0712, -0.0919, -0.0027, -0.1690, -0.0875, -0.1343,
                           0.1530, -0.0886, -0.0698,  0.1540,  0.1790, -0.0707,  0.1121,  0.1836,
                           0.1868, -0.0792,  0.2077, -0.0793, -0.1420,  0.0465, -0.0088,  0.1172,
                          -0.0367, -0.1313, -0.0480,  0.0676, -0.1355,  0.0982,  0.1367, -0.0494,
                          -0.0798,  0.0952,  0.1556,  0.0431,  0.1364,  0.0783,  0.1828,  0.0923,
                          -0.1137,  0.1838,  0.1636,  0.1233,  0.1776, -0.1573,  0.0854,  0.0007,
                           0.1211,  0.0110, -0.2182, -0.0487, -0.0093,  0.2103, -0.2203, -0.1809,
                           0.0250, -0.1747, -0.0134,  0.1462,  0.0606,  0.0120,  0.1267, -0.1504,
                           0.0651,  0.2034, -0.1150, -0.1709, -0.1697,  0.0113, -0.0758,  0.1178,
                           0.0164, -0.1215, -0.0651,  0.0680, -0.1631,  0.0755, -0.1190,  0.0384,
                          -0.1965,  0.0182, -0.0724, -0.0316,  0.0176, -0.1655,  0.0507, -0.1219,
                           0.1202,  0.0254,  0.1698, -0.0890,  0.1968, -0.1419, -0.1922,  0.0703,
                           0.1278,  0.0852,  0.0899, -0.1138, -0.0535, -0.0802, -0.1164,  0.1638,
                          -0.1939,  0.1950, -0.1853, -0.2065,  0.1341,  0.0865,  0.1317,  0.0514,
                          -0.0098, -0.0319, -0.0282,  0.1304,  0.1279,  0.0616,  0.0902,  0.1834],
                         device='cuda:0')),
                 ('output.weight',
                  tensor([[ 0.0378,  0.0368, -0.0586,  ...,  0.0592, -0.0195, -0.0052],
                          [-0.0527, -0.0614, -0.0420,  ...,  0.0054, -0.0490,  0.0007],
                          [ 0.0558, -0.0129,  0.0006,  ..., -0.0409,  0.0259,  0.0535],
                          ...,
                          [-0.0624, -0.0108,  0.0038,  ..., -0.0055,  0.0586,  0.0554],
                          [ 0.0592,  0.0014, -0.0083,  ..., -0.0480, -0.0523, -0.0138],
                          [ 0.0562,  0.0339,  0.0364,  ...,  0.0122,  0.0533, -0.0364]],
                         device='cuda:0')),
                 ('output.bias',
                  tensor([ 0.0610, -0.0384, -0.0256, -0.0007,  0.0381, -0.0479, -0.0177, -0.0192,
                           0.0078, -0.0304], device='cuda:0'))])



### 练习 5.6.3 

测量计算1000个$100 \times 100$矩阵的矩阵乘法所需的时间，并记录输出矩阵的Frobenius范数，一次记录一个结果，而不是在GPU上保存日志并仅传输最终结果。

**解答:** 

&emsp;&emsp;中文版翻译有点问题，英文原版这句话是：

>&emsp;&emsp;Measure the time it takes to compute 1000 matrix-matrix multiplications of $100×100$ matrices and log the Frobenius norm of the output matrix one result at a time vs. keeping a log on the GPU and transferring only the final result.

&emsp;&emsp;所以这道题的本质还是希望我们做个比较。

&emsp;&emsp;实验一：仅记录1000次$100×100$矩阵相乘所用的时间，不需要打印Frobenius范数。

&emsp;&emsp;实验二：记录1000次$100×100$矩阵相乘所用的时间，并打印Frobenius范数。


```python
import torch
import time

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# 生成随机矩阵
matrices = [torch.randn(100, 100).to(device) for i in range(1000)]

# 实验一：计算时间
start_time_1 = time.time()
for i in range(1000):
    result = torch.mm(matrices[i], matrices[i].t())
    frobenius_norm = torch.norm(result)
#     print(frobenius_norm)
end_time_1 = time.time()
print("Time taken:", end_time_1 - start_time_1)

# 实验二：计算时间
start_time_2 = time.time()
for i in range(1000):
    result = torch.mm(matrices[i], matrices[i].t())
    frobenius_norm = torch.norm(result)
    print(frobenius_norm)
end_time_2 = time.time()
print("Time taken:", end_time_2 - start_time_2)

print(f'实验一消耗时间：{end_time_1 - start_time_1}，实验二消耗时间：{end_time_2 - start_time_2}')
```

    Time taken: 0.083892822265625
    tensor(1414.3264, device='cuda:0')
    tensor(1420.3613, device='cuda:0')
    tensor(1426.7102, device='cuda:0')
    tensor(1404.4108, device='cuda:0')
    tensor(1399.9667, device='cuda:0')
    tensor(1423.5759, device='cuda:0')
    tensor(1404.5486, device='cuda:0')
    tensor(1386.4878, device='cuda:0')
    tensor(1453.0780, device='cuda:0')
    tensor(1399.2872, device='cuda:0')
    tensor(1401.3347, device='cuda:0')
    tensor(1442.3737, device='cuda:0')
    tensor(1409.5651, device='cuda:0')
    tensor(1412.1494, device='cuda:0')
    tensor(1399.0287, device='cuda:0')
    tensor(1377.4283, device='cuda:0')
    tensor(1409.3846, device='cuda:0')
    tensor(1440.5291, device='cuda:0')
    tensor(1383.9038, device='cuda:0')
    tensor(1401.8132, device='cuda:0')
    tensor(1444.8940, device='cuda:0')
    tensor(1407.5876, device='cuda:0')
    tensor(1392.9998, device='cuda:0')
    tensor(1447.4484, device='cuda:0')
    tensor(1441.7650, device='cuda:0')
    tensor(1424.2852, device='cuda:0')
    tensor(1412.0476, device='cuda:0')
    tensor(1446.4708, device='cuda:0')
    tensor(1439.6016, device='cuda:0')
    tensor(1400.5818, device='cuda:0')
    tensor(1442.8463, device='cuda:0')
    tensor(1401.3495, device='cuda:0')
    tensor(1416.1384, device='cuda:0')
    tensor(1407.5288, device='cuda:0')
    tensor(1400.6179, device='cuda:0')
    tensor(1439.5996, device='cuda:0')
    tensor(1390.1395, device='cuda:0')
    tensor(1424.9067, device='cuda:0')
    tensor(1419.1511, device='cuda:0')
    tensor(1397.7457, device='cuda:0')
    tensor(1424.5667, device='cuda:0')
    tensor(1431.6057, device='cuda:0')
    tensor(1414.7839, device='cuda:0')
    tensor(1404.5869, device='cuda:0')
    tensor(1410.4812, device='cuda:0')
    tensor(1426.4489, device='cuda:0')
    tensor(1395.1388, device='cuda:0')
    tensor(1432.4199, device='cuda:0')
    tensor(1388.4635, device='cuda:0')
    tensor(1412.4142, device='cuda:0')
    tensor(1409.0317, device='cuda:0')
    tensor(1444.3677, device='cuda:0')
    tensor(1425.9957, device='cuda:0')
    tensor(1399.0273, device='cuda:0')
    tensor(1391.0610, device='cuda:0')
    tensor(1422.5696, device='cuda:0')
    tensor(1385.3455, device='cuda:0')
    tensor(1405.2266, device='cuda:0')
    tensor(1414.0631, device='cuda:0')
    tensor(1441.1052, device='cuda:0')
    tensor(1404.9672, device='cuda:0')
    tensor(1434.1139, device='cuda:0')
    tensor(1382.1471, device='cuda:0')
    tensor(1431.5863, device='cuda:0')
    tensor(1393.2568, device='cuda:0')
    tensor(1398.7456, device='cuda:0')
    tensor(1452.7546, device='cuda:0')
    tensor(1422.0884, device='cuda:0')
    tensor(1426.9169, device='cuda:0')
    tensor(1406.6737, device='cuda:0')
    tensor(1415.6372, device='cuda:0')
    tensor(1425.7087, device='cuda:0')
    tensor(1436.4384, device='cuda:0')
    tensor(1408.1709, device='cuda:0')
    tensor(1397.7487, device='cuda:0')
    tensor(1432.1602, device='cuda:0')
    tensor(1401.4945, device='cuda:0')
    tensor(1364.3336, device='cuda:0')
    tensor(1432.4294, device='cuda:0')
    tensor(1420.6379, device='cuda:0')
    tensor(1402.3220, device='cuda:0')
    tensor(1415.9830, device='cuda:0')
    tensor(1427.9037, device='cuda:0')
    tensor(1426.5095, device='cuda:0')
    tensor(1404.6687, device='cuda:0')
    tensor(1412.2659, device='cuda:0')
    tensor(1425.6060, device='cuda:0')
    tensor(1455.0330, device='cuda:0')
    tensor(1425.2367, device='cuda:0')
    tensor(1412.7449, device='cuda:0')
    tensor(1383.8060, device='cuda:0')
    tensor(1462.7919, device='cuda:0')
    tensor(1441.7439, device='cuda:0')
    tensor(1428.8705, device='cuda:0')
    tensor(1388.8622, device='cuda:0')
    tensor(1409.4866, device='cuda:0')
    tensor(1408.3340, device='cuda:0')
    tensor(1409.9652, device='cuda:0')
    tensor(1428.4456, device='cuda:0')
    tensor(1458.7307, device='cuda:0')
    tensor(1412.8656, device='cuda:0')
    tensor(1398.3041, device='cuda:0')
    tensor(1454.4297, device='cuda:0')
    tensor(1420.5155, device='cuda:0')
    tensor(1409.4517, device='cuda:0')
    tensor(1437.3512, device='cuda:0')
    tensor(1439.3440, device='cuda:0')
    tensor(1420.8407, device='cuda:0')
    tensor(1385.2998, device='cuda:0')
    tensor(1403.2085, device='cuda:0')
    tensor(1443.1156, device='cuda:0')
    tensor(1436.5155, device='cuda:0')
    tensor(1412.8850, device='cuda:0')
    tensor(1430.8556, device='cuda:0')
    tensor(1426.4655, device='cuda:0')
    tensor(1437.0756, device='cuda:0')
    tensor(1448.4524, device='cuda:0')
    tensor(1406.4270, device='cuda:0')
    tensor(1416.3265, device='cuda:0')
    tensor(1385.5703, device='cuda:0')
    tensor(1409.6227, device='cuda:0')
    tensor(1387.6338, device='cuda:0')
    tensor(1387.0051, device='cuda:0')
    tensor(1401.6917, device='cuda:0')
    tensor(1459.0684, device='cuda:0')
    tensor(1415.2648, device='cuda:0')
    tensor(1400.3497, device='cuda:0')
    tensor(1427.4443, device='cuda:0')
    tensor(1427.0580, device='cuda:0')
    tensor(1434.4236, device='cuda:0')
    tensor(1428.7977, device='cuda:0')
    tensor(1415.7834, device='cuda:0')
    tensor(1419.2070, device='cuda:0')
    tensor(1450.7157, device='cuda:0')
    tensor(1426.0618, device='cuda:0')
    tensor(1411.4818, device='cuda:0')
    tensor(1393.3303, device='cuda:0')
    tensor(1429.1759, device='cuda:0')
    tensor(1438.4055, device='cuda:0')
    tensor(1382.0458, device='cuda:0')
    tensor(1414.6213, device='cuda:0')
    tensor(1416.8164, device='cuda:0')
    tensor(1402.7170, device='cuda:0')
    tensor(1424.0973, device='cuda:0')
    tensor(1396.5586, device='cuda:0')
    tensor(1397.8713, device='cuda:0')
    tensor(1413.1685, device='cuda:0')
    tensor(1420.2782, device='cuda:0')
    tensor(1451.3582, device='cuda:0')
    tensor(1389.3899, device='cuda:0')
    tensor(1417.5210, device='cuda:0')
    tensor(1405.4589, device='cuda:0')
    tensor(1418.8301, device='cuda:0')
    tensor(1441.7411, device='cuda:0')
    tensor(1393.5591, device='cuda:0')
    tensor(1459.0192, device='cuda:0')
    tensor(1384.6997, device='cuda:0')
    tensor(1367.1498, device='cuda:0')
    tensor(1411.3000, device='cuda:0')
    tensor(1449.4939, device='cuda:0')
    tensor(1432.4763, device='cuda:0')
    tensor(1407.5201, device='cuda:0')
    tensor(1423.5007, device='cuda:0')
    tensor(1421.6755, device='cuda:0')
    tensor(1407.4553, device='cuda:0')
    tensor(1396.2762, device='cuda:0')
    tensor(1438.9589, device='cuda:0')
    tensor(1426.8402, device='cuda:0')
    tensor(1426.7487, device='cuda:0')
    tensor(1408.7693, device='cuda:0')
    tensor(1412.2242, device='cuda:0')
    tensor(1390.1692, device='cuda:0')
    tensor(1397.5320, device='cuda:0')
    tensor(1418.8942, device='cuda:0')
    tensor(1409.5818, device='cuda:0')
    tensor(1450.8793, device='cuda:0')
    tensor(1445.1687, device='cuda:0')
    tensor(1436.6711, device='cuda:0')
    tensor(1380.7662, device='cuda:0')
    tensor(1420.0610, device='cuda:0')
    tensor(1449.6732, device='cuda:0')
    tensor(1410.6570, device='cuda:0')
    tensor(1399.0934, device='cuda:0')
    tensor(1402.6781, device='cuda:0')
    tensor(1419.4244, device='cuda:0')
    tensor(1431.7424, device='cuda:0')
    tensor(1425.0427, device='cuda:0')
    tensor(1422.3596, device='cuda:0')
    tensor(1446.0736, device='cuda:0')
    tensor(1393.0781, device='cuda:0')
    tensor(1403.0770, device='cuda:0')
    tensor(1420.0957, device='cuda:0')
    tensor(1443.6151, device='cuda:0')
    tensor(1412.4930, device='cuda:0')
    tensor(1459.8573, device='cuda:0')
    tensor(1405.6935, device='cuda:0')
    tensor(1372.5243, device='cuda:0')
    tensor(1411.5518, device='cuda:0')
    tensor(1391.6947, device='cuda:0')
    tensor(1415.2728, device='cuda:0')
    tensor(1444.5618, device='cuda:0')
    tensor(1408.9080, device='cuda:0')
    tensor(1392.5614, device='cuda:0')
    tensor(1427.8918, device='cuda:0')
    tensor(1419.5483, device='cuda:0')
    tensor(1415.6810, device='cuda:0')
    tensor(1397.2209, device='cuda:0')
    tensor(1396.9866, device='cuda:0')
    tensor(1426.4923, device='cuda:0')
    tensor(1456.2041, device='cuda:0')
    tensor(1369.0884, device='cuda:0')
    tensor(1405.4847, device='cuda:0')
    tensor(1434.1417, device='cuda:0')
    tensor(1429.5665, device='cuda:0')
    tensor(1471.4082, device='cuda:0')
    tensor(1424.3124, device='cuda:0')
    tensor(1403.1338, device='cuda:0')
    tensor(1405.5498, device='cuda:0')
    tensor(1450.0192, device='cuda:0')
    tensor(1415.6967, device='cuda:0')
    tensor(1396.4835, device='cuda:0')
    tensor(1420.1073, device='cuda:0')
    tensor(1467.6332, device='cuda:0')
    tensor(1442.1469, device='cuda:0')
    tensor(1435.9219, device='cuda:0')
    tensor(1429.7675, device='cuda:0')
    tensor(1382.4639, device='cuda:0')
    tensor(1421.8973, device='cuda:0')
    tensor(1385.1857, device='cuda:0')
    tensor(1421.3765, device='cuda:0')
    tensor(1399.2683, device='cuda:0')
    tensor(1415.0687, device='cuda:0')
    tensor(1429.7700, device='cuda:0')
    tensor(1387.0018, device='cuda:0')
    tensor(1431.9097, device='cuda:0')
    tensor(1417.7034, device='cuda:0')
    tensor(1443.3807, device='cuda:0')
    tensor(1383.3411, device='cuda:0')
    tensor(1453.2593, device='cuda:0')
    tensor(1443.6071, device='cuda:0')
    tensor(1407.7806, device='cuda:0')
    tensor(1460.8347, device='cuda:0')
    tensor(1385.2125, device='cuda:0')
    tensor(1442.6860, device='cuda:0')
    tensor(1435.8127, device='cuda:0')
    tensor(1385.6194, device='cuda:0')
    tensor(1423.0961, device='cuda:0')
    tensor(1397.1876, device='cuda:0')
    tensor(1429.1403, device='cuda:0')
    tensor(1383.8939, device='cuda:0')
    tensor(1420.3212, device='cuda:0')
    tensor(1428.1782, device='cuda:0')
    tensor(1405.0005, device='cuda:0')
    tensor(1403.5986, device='cuda:0')
    tensor(1366.1530, device='cuda:0')
    tensor(1409.2677, device='cuda:0')
    tensor(1393.9169, device='cuda:0')
    tensor(1411.2216, device='cuda:0')
    tensor(1401.5234, device='cuda:0')
    tensor(1419.8258, device='cuda:0')
    tensor(1402.1510, device='cuda:0')
    tensor(1382.2053, device='cuda:0')
    tensor(1428.4900, device='cuda:0')
    tensor(1432.6130, device='cuda:0')
    tensor(1432.9546, device='cuda:0')
    tensor(1402.5962, device='cuda:0')
    tensor(1414.0640, device='cuda:0')
    tensor(1390.7435, device='cuda:0')
    tensor(1421.4161, device='cuda:0')
    tensor(1396.6892, device='cuda:0')
    tensor(1438.0793, device='cuda:0')
    tensor(1434.9421, device='cuda:0')
    tensor(1422.1665, device='cuda:0')
    tensor(1381.1476, device='cuda:0')
    tensor(1420.3358, device='cuda:0')
    tensor(1404.7455, device='cuda:0')
    tensor(1395.9352, device='cuda:0')
    tensor(1432.2086, device='cuda:0')
    tensor(1462.7440, device='cuda:0')
    tensor(1455.9264, device='cuda:0')
    tensor(1434.2323, device='cuda:0')
    tensor(1412.7642, device='cuda:0')
    tensor(1416.1359, device='cuda:0')
    tensor(1436.3552, device='cuda:0')
    tensor(1424.6135, device='cuda:0')
    tensor(1405.4802, device='cuda:0')
    tensor(1382.1775, device='cuda:0')
    tensor(1444.7761, device='cuda:0')
    tensor(1391.2672, device='cuda:0')
    tensor(1407.9937, device='cuda:0')
    tensor(1444.0236, device='cuda:0')
    tensor(1425.7430, device='cuda:0')
    tensor(1434.1818, device='cuda:0')
    tensor(1388.1573, device='cuda:0')
    tensor(1378.3517, device='cuda:0')
    tensor(1433.2560, device='cuda:0')
    tensor(1413.1733, device='cuda:0')
    tensor(1425.3997, device='cuda:0')
    tensor(1415.9795, device='cuda:0')
    tensor(1426.1742, device='cuda:0')
    tensor(1401.4117, device='cuda:0')
    tensor(1400.5546, device='cuda:0')
    tensor(1409.5291, device='cuda:0')
    tensor(1424.6991, device='cuda:0')
    tensor(1404.9939, device='cuda:0')
    tensor(1416.3623, device='cuda:0')
    tensor(1413.5933, device='cuda:0')
    tensor(1414.0509, device='cuda:0')
    tensor(1419.7173, device='cuda:0')
    tensor(1429.9849, device='cuda:0')
    tensor(1408.4830, device='cuda:0')
    tensor(1403.2603, device='cuda:0')
    tensor(1394.5729, device='cuda:0')
    tensor(1414.0874, device='cuda:0')
    tensor(1465.8975, device='cuda:0')
    tensor(1451.1857, device='cuda:0')
    tensor(1470.9673, device='cuda:0')
    tensor(1380.7628, device='cuda:0')
    tensor(1412.7468, device='cuda:0')
    tensor(1426.3804, device='cuda:0')
    tensor(1405.4205, device='cuda:0')
    tensor(1428.5094, device='cuda:0')
    tensor(1412.5598, device='cuda:0')
    tensor(1417.3813, device='cuda:0')
    tensor(1426.7632, device='cuda:0')
    tensor(1396.8458, device='cuda:0')
    tensor(1390.5662, device='cuda:0')
    tensor(1465.5986, device='cuda:0')
    tensor(1368.3411, device='cuda:0')
    tensor(1451.1282, device='cuda:0')
    tensor(1451.7244, device='cuda:0')
    tensor(1389.8445, device='cuda:0')
    tensor(1444.2655, device='cuda:0')
    tensor(1416.1716, device='cuda:0')
    tensor(1429.8531, device='cuda:0')
    tensor(1467.9452, device='cuda:0')
    tensor(1447.1864, device='cuda:0')
    tensor(1409.8638, device='cuda:0')
    tensor(1420.3434, device='cuda:0')
    tensor(1423.9861, device='cuda:0')
    tensor(1429.1010, device='cuda:0')
    tensor(1439.2847, device='cuda:0')
    tensor(1466.9221, device='cuda:0')
    tensor(1438.8833, device='cuda:0')
    tensor(1433.8713, device='cuda:0')
    tensor(1416.6282, device='cuda:0')
    tensor(1400.9237, device='cuda:0')
    tensor(1418.0923, device='cuda:0')
    tensor(1433.3290, device='cuda:0')
    tensor(1429.4817, device='cuda:0')
    tensor(1418.6892, device='cuda:0')
    tensor(1412.2926, device='cuda:0')
    tensor(1415.1140, device='cuda:0')
    tensor(1427.6580, device='cuda:0')
    tensor(1470.7190, device='cuda:0')
    tensor(1428.6884, device='cuda:0')
    tensor(1449.6207, device='cuda:0')
    tensor(1404.9960, device='cuda:0')
    tensor(1410.9005, device='cuda:0')
    tensor(1405.3414, device='cuda:0')
    tensor(1374.9852, device='cuda:0')
    tensor(1398.2344, device='cuda:0')
    tensor(1365.1029, device='cuda:0')
    tensor(1384.9991, device='cuda:0')
    tensor(1418.4922, device='cuda:0')
    tensor(1397.8925, device='cuda:0')
    tensor(1421.8087, device='cuda:0')
    tensor(1409.1899, device='cuda:0')
    tensor(1409.6622, device='cuda:0')
    tensor(1437.6113, device='cuda:0')
    tensor(1435.6271, device='cuda:0')
    tensor(1424.1324, device='cuda:0')
    tensor(1407.0251, device='cuda:0')
    tensor(1398.9154, device='cuda:0')
    tensor(1402.5077, device='cuda:0')
    tensor(1401.2939, device='cuda:0')
    tensor(1360.6781, device='cuda:0')
    tensor(1454.7137, device='cuda:0')
    tensor(1454.7367, device='cuda:0')
    tensor(1388.7833, device='cuda:0')
    tensor(1408.0422, device='cuda:0')
    tensor(1432.1945, device='cuda:0')
    tensor(1404.3351, device='cuda:0')
    tensor(1418.7017, device='cuda:0')
    tensor(1432.2267, device='cuda:0')
    tensor(1421.2924, device='cuda:0')
    tensor(1384.1738, device='cuda:0')
    tensor(1437.2968, device='cuda:0')
    tensor(1423.3580, device='cuda:0')
    tensor(1404.5396, device='cuda:0')
    tensor(1444.3097, device='cuda:0')
    tensor(1401.6013, device='cuda:0')
    tensor(1412.9150, device='cuda:0')
    tensor(1422.6688, device='cuda:0')
    tensor(1422.4749, device='cuda:0')
    tensor(1428.0391, device='cuda:0')
    tensor(1402.7891, device='cuda:0')
    tensor(1403.7410, device='cuda:0')
    tensor(1403.7869, device='cuda:0')
    tensor(1420.3461, device='cuda:0')
    tensor(1423.8771, device='cuda:0')
    tensor(1430.0901, device='cuda:0')
    tensor(1432.7458, device='cuda:0')
    tensor(1375.4868, device='cuda:0')
    tensor(1413.0044, device='cuda:0')
    tensor(1418.3054, device='cuda:0')
    tensor(1426.6083, device='cuda:0')
    tensor(1380.1199, device='cuda:0')
    tensor(1392.9200, device='cuda:0')
    tensor(1439.8191, device='cuda:0')
    tensor(1391.5569, device='cuda:0')
    tensor(1415.3403, device='cuda:0')
    tensor(1394.4769, device='cuda:0')
    tensor(1411.8767, device='cuda:0')
    tensor(1387.7863, device='cuda:0')
    tensor(1417.2864, device='cuda:0')
    tensor(1408.4110, device='cuda:0')
    tensor(1413.7633, device='cuda:0')
    tensor(1418.6460, device='cuda:0')
    tensor(1426.2806, device='cuda:0')
    tensor(1411.6372, device='cuda:0')
    tensor(1402.4010, device='cuda:0')
    tensor(1397.1362, device='cuda:0')
    tensor(1439.5264, device='cuda:0')
    tensor(1437.9934, device='cuda:0')
    tensor(1414.0233, device='cuda:0')
    tensor(1407.7537, device='cuda:0')
    tensor(1402.5347, device='cuda:0')
    tensor(1408.2753, device='cuda:0')
    tensor(1438.6881, device='cuda:0')
    tensor(1408.5067, device='cuda:0')
    tensor(1439.3190, device='cuda:0')
    tensor(1435.7350, device='cuda:0')
    tensor(1407.3895, device='cuda:0')
    tensor(1399.8881, device='cuda:0')
    tensor(1422.3414, device='cuda:0')
    tensor(1408.3064, device='cuda:0')
    tensor(1365.7664, device='cuda:0')
    tensor(1433.2239, device='cuda:0')
    tensor(1424.7485, device='cuda:0')
    tensor(1420.8862, device='cuda:0')
    tensor(1426.5361, device='cuda:0')
    tensor(1386.8502, device='cuda:0')
    tensor(1430.3644, device='cuda:0')
    tensor(1409.7175, device='cuda:0')
    tensor(1455.3163, device='cuda:0')
    tensor(1439.4346, device='cuda:0')
    tensor(1400.4198, device='cuda:0')
    tensor(1374.9525, device='cuda:0')
    tensor(1437.6892, device='cuda:0')
    tensor(1425.2881, device='cuda:0')
    tensor(1395.4740, device='cuda:0')
    tensor(1436.3489, device='cuda:0')
    tensor(1402.9910, device='cuda:0')
    tensor(1407.4963, device='cuda:0')
    tensor(1405.6246, device='cuda:0')
    tensor(1435.1608, device='cuda:0')
    tensor(1390.6985, device='cuda:0')
    tensor(1442.0021, device='cuda:0')
    tensor(1441.5020, device='cuda:0')
    tensor(1426.5122, device='cuda:0')
    tensor(1473.3478, device='cuda:0')
    tensor(1416.2357, device='cuda:0')
    tensor(1421.5352, device='cuda:0')
    tensor(1431.3108, device='cuda:0')
    tensor(1415.8021, device='cuda:0')
    tensor(1395.7290, device='cuda:0')
    tensor(1422.5176, device='cuda:0')
    tensor(1430.6995, device='cuda:0')
    tensor(1417.5221, device='cuda:0')
    tensor(1413.3586, device='cuda:0')
    tensor(1408.5188, device='cuda:0')
    tensor(1414.3170, device='cuda:0')
    tensor(1447.1042, device='cuda:0')
    tensor(1385.1370, device='cuda:0')
    tensor(1418.7260, device='cuda:0')
    tensor(1380.2251, device='cuda:0')
    tensor(1404.6027, device='cuda:0')
    tensor(1417.0959, device='cuda:0')
    tensor(1439.8796, device='cuda:0')
    tensor(1417.5627, device='cuda:0')
    tensor(1433.6448, device='cuda:0')
    tensor(1413.6857, device='cuda:0')
    tensor(1436.9205, device='cuda:0')
    tensor(1425.8083, device='cuda:0')
    tensor(1433.5669, device='cuda:0')
    tensor(1395.6774, device='cuda:0')
    tensor(1384.6754, device='cuda:0')
    tensor(1405.2471, device='cuda:0')
    tensor(1401.4568, device='cuda:0')
    tensor(1426.3107, device='cuda:0')
    tensor(1439.5176, device='cuda:0')
    tensor(1419.9882, device='cuda:0')
    tensor(1380.9194, device='cuda:0')
    tensor(1413.2659, device='cuda:0')
    tensor(1411.4631, device='cuda:0')
    tensor(1439.6840, device='cuda:0')
    tensor(1385.5994, device='cuda:0')
    tensor(1429.2371, device='cuda:0')
    tensor(1394.6469, device='cuda:0')
    tensor(1439.3892, device='cuda:0')
    tensor(1415.7286, device='cuda:0')
    tensor(1391.0919, device='cuda:0')
    tensor(1382.4852, device='cuda:0')
    tensor(1386.4874, device='cuda:0')
    tensor(1422.5387, device='cuda:0')
    tensor(1447.5110, device='cuda:0')
    tensor(1416.0176, device='cuda:0')
    tensor(1400.7793, device='cuda:0')
    tensor(1398.1163, device='cuda:0')
    tensor(1427.2731, device='cuda:0')
    tensor(1425.4888, device='cuda:0')
    tensor(1439.1068, device='cuda:0')
    tensor(1433.0099, device='cuda:0')
    tensor(1408.9768, device='cuda:0')
    tensor(1389.0740, device='cuda:0')
    tensor(1405.4551, device='cuda:0')
    tensor(1417.8910, device='cuda:0')
    tensor(1392.1174, device='cuda:0')
    tensor(1416.3024, device='cuda:0')
    tensor(1441.5596, device='cuda:0')
    tensor(1418.6179, device='cuda:0')
    tensor(1421.4349, device='cuda:0')
    tensor(1468.2260, device='cuda:0')
    tensor(1448.2383, device='cuda:0')
    tensor(1412.8549, device='cuda:0')
    tensor(1443.3477, device='cuda:0')
    tensor(1416.3853, device='cuda:0')
    tensor(1406.0111, device='cuda:0')
    tensor(1443.9180, device='cuda:0')
    tensor(1425.6541, device='cuda:0')
    tensor(1413.3318, device='cuda:0')
    tensor(1421.2944, device='cuda:0')
    tensor(1419.3005, device='cuda:0')
    tensor(1430.3191, device='cuda:0')
    tensor(1412.7074, device='cuda:0')
    tensor(1421.5516, device='cuda:0')
    tensor(1413.7882, device='cuda:0')
    tensor(1436.7692, device='cuda:0')
    tensor(1425.3563, device='cuda:0')
    tensor(1417.0079, device='cuda:0')
    tensor(1403.8787, device='cuda:0')
    tensor(1441.4609, device='cuda:0')
    tensor(1414.7581, device='cuda:0')
    tensor(1414.9227, device='cuda:0')
    tensor(1417.8657, device='cuda:0')
    tensor(1446.0958, device='cuda:0')
    tensor(1402.4999, device='cuda:0')
    tensor(1414.6422, device='cuda:0')
    tensor(1410.7300, device='cuda:0')
    tensor(1400.5583, device='cuda:0')
    tensor(1426.6515, device='cuda:0')
    tensor(1443.7634, device='cuda:0')
    tensor(1426.3607, device='cuda:0')
    tensor(1402.7223, device='cuda:0')
    tensor(1415.4741, device='cuda:0')
    tensor(1404.5396, device='cuda:0')
    tensor(1414.0891, device='cuda:0')
    tensor(1420.7969, device='cuda:0')
    tensor(1396.9275, device='cuda:0')
    tensor(1458.3097, device='cuda:0')
    tensor(1434.4164, device='cuda:0')
    tensor(1393.2557, device='cuda:0')
    tensor(1441.0255, device='cuda:0')
    tensor(1380.6505, device='cuda:0')
    tensor(1409.9803, device='cuda:0')
    tensor(1368.8228, device='cuda:0')
    tensor(1400.6229, device='cuda:0')
    tensor(1415.9321, device='cuda:0')
    tensor(1424.6715, device='cuda:0')
    tensor(1385.3785, device='cuda:0')
    tensor(1417.8522, device='cuda:0')
    tensor(1421.3883, device='cuda:0')
    tensor(1449.0311, device='cuda:0')
    tensor(1426.6940, device='cuda:0')
    tensor(1438.8097, device='cuda:0')
    tensor(1407.8922, device='cuda:0')
    tensor(1435.3757, device='cuda:0')
    tensor(1455.0043, device='cuda:0')
    tensor(1391.4935, device='cuda:0')
    tensor(1417.1824, device='cuda:0')
    tensor(1428.1558, device='cuda:0')
    tensor(1431.2241, device='cuda:0')
    tensor(1420.5492, device='cuda:0')
    tensor(1425.2817, device='cuda:0')
    tensor(1389.0997, device='cuda:0')
    tensor(1409.1003, device='cuda:0')
    tensor(1409.8082, device='cuda:0')
    tensor(1379.0896, device='cuda:0')
    tensor(1408.6285, device='cuda:0')
    tensor(1383.6857, device='cuda:0')
    tensor(1410.5594, device='cuda:0')
    tensor(1400.9836, device='cuda:0')
    tensor(1405.1199, device='cuda:0')
    tensor(1449.0201, device='cuda:0')
    tensor(1381.7177, device='cuda:0')
    tensor(1435.9558, device='cuda:0')
    tensor(1395.7903, device='cuda:0')
    tensor(1414.4789, device='cuda:0')
    tensor(1418.1703, device='cuda:0')
    tensor(1439.0444, device='cuda:0')
    tensor(1433.3157, device='cuda:0')
    tensor(1450.6144, device='cuda:0')
    tensor(1382.4734, device='cuda:0')
    tensor(1421.0452, device='cuda:0')
    tensor(1432.8387, device='cuda:0')
    tensor(1416.5317, device='cuda:0')
    tensor(1449.9954, device='cuda:0')
    tensor(1382.6801, device='cuda:0')
    tensor(1448.8501, device='cuda:0')
    tensor(1417.5383, device='cuda:0')
    tensor(1430.1782, device='cuda:0')
    tensor(1435.8892, device='cuda:0')
    tensor(1367.6129, device='cuda:0')
    tensor(1384.8370, device='cuda:0')
    tensor(1377.2517, device='cuda:0')
    tensor(1392.1387, device='cuda:0')
    tensor(1412.3425, device='cuda:0')
    tensor(1441.9358, device='cuda:0')
    tensor(1447.9532, device='cuda:0')
    tensor(1404.0717, device='cuda:0')
    tensor(1424.9260, device='cuda:0')
    tensor(1428.9032, device='cuda:0')
    tensor(1444.6342, device='cuda:0')
    tensor(1446.0267, device='cuda:0')
    tensor(1434.1827, device='cuda:0')
    tensor(1392.0759, device='cuda:0')
    tensor(1375.9364, device='cuda:0')
    tensor(1442.5314, device='cuda:0')
    tensor(1428.5896, device='cuda:0')
    tensor(1378.6542, device='cuda:0')
    tensor(1398.1370, device='cuda:0')
    tensor(1400.0981, device='cuda:0')
    tensor(1431.5441, device='cuda:0')
    tensor(1403.0608, device='cuda:0')
    tensor(1440.6503, device='cuda:0')
    tensor(1429.8313, device='cuda:0')
    tensor(1441.6162, device='cuda:0')
    tensor(1391.8026, device='cuda:0')
    tensor(1425.2073, device='cuda:0')
    tensor(1423.4188, device='cuda:0')
    tensor(1389.9656, device='cuda:0')
    tensor(1411.0577, device='cuda:0')
    tensor(1423.1200, device='cuda:0')
    tensor(1401.7646, device='cuda:0')
    tensor(1408.5344, device='cuda:0')
    tensor(1412.2050, device='cuda:0')
    tensor(1436.6595, device='cuda:0')
    tensor(1407.4791, device='cuda:0')
    tensor(1394.5499, device='cuda:0')
    tensor(1413.3774, device='cuda:0')
    tensor(1415.2762, device='cuda:0')
    tensor(1453.4368, device='cuda:0')
    tensor(1434.2491, device='cuda:0')
    tensor(1388.5255, device='cuda:0')
    tensor(1383.2640, device='cuda:0')
    tensor(1424.7712, device='cuda:0')
    tensor(1397.3434, device='cuda:0')
    tensor(1418.6311, device='cuda:0')
    tensor(1402.5612, device='cuda:0')
    tensor(1440.5050, device='cuda:0')
    tensor(1402.3740, device='cuda:0')
    tensor(1408.8771, device='cuda:0')
    tensor(1420.6165, device='cuda:0')
    tensor(1427.5487, device='cuda:0')
    tensor(1410.8777, device='cuda:0')
    tensor(1383.2748, device='cuda:0')
    tensor(1395.0757, device='cuda:0')
    tensor(1393.4822, device='cuda:0')
    tensor(1393.1779, device='cuda:0')
    tensor(1441.6558, device='cuda:0')
    tensor(1444.5702, device='cuda:0')
    tensor(1421.3900, device='cuda:0')
    tensor(1431.2898, device='cuda:0')
    tensor(1430.2230, device='cuda:0')
    tensor(1386.1057, device='cuda:0')
    tensor(1405.4774, device='cuda:0')
    tensor(1450.7740, device='cuda:0')
    tensor(1401.5381, device='cuda:0')
    tensor(1438.6777, device='cuda:0')
    tensor(1416.8682, device='cuda:0')
    tensor(1404.4166, device='cuda:0')
    tensor(1412.6957, device='cuda:0')
    tensor(1429.1792, device='cuda:0')
    tensor(1447.5914, device='cuda:0')
    tensor(1384.9110, device='cuda:0')
    tensor(1375.4346, device='cuda:0')
    tensor(1417.9133, device='cuda:0')
    tensor(1425.8375, device='cuda:0')
    tensor(1422.2826, device='cuda:0')
    tensor(1416.1715, device='cuda:0')
    tensor(1448.8301, device='cuda:0')
    tensor(1426.6920, device='cuda:0')
    tensor(1428.4994, device='cuda:0')
    tensor(1429.9166, device='cuda:0')
    tensor(1403.6028, device='cuda:0')
    tensor(1366.8010, device='cuda:0')
    tensor(1396.8983, device='cuda:0')
    tensor(1446.0986, device='cuda:0')
    tensor(1402.0897, device='cuda:0')
    tensor(1417.4302, device='cuda:0')
    tensor(1395.6691, device='cuda:0')
    tensor(1426.8137, device='cuda:0')
    tensor(1409.6072, device='cuda:0')
    tensor(1425.8777, device='cuda:0')
    tensor(1395.2622, device='cuda:0')
    tensor(1450.3542, device='cuda:0')
    tensor(1416.1274, device='cuda:0')
    tensor(1428.4930, device='cuda:0')
    tensor(1414.3604, device='cuda:0')
    tensor(1418.1160, device='cuda:0')
    tensor(1410.1843, device='cuda:0')
    tensor(1406.0295, device='cuda:0')
    tensor(1390.4836, device='cuda:0')
    tensor(1416.7998, device='cuda:0')
    tensor(1420.3181, device='cuda:0')
    tensor(1414.9705, device='cuda:0')
    tensor(1441.2191, device='cuda:0')
    tensor(1432.8761, device='cuda:0')
    tensor(1409.1808, device='cuda:0')
    tensor(1423.1477, device='cuda:0')
    tensor(1381.2469, device='cuda:0')
    tensor(1382.9114, device='cuda:0')
    tensor(1367.7100, device='cuda:0')
    tensor(1408.9882, device='cuda:0')
    tensor(1422.5929, device='cuda:0')
    tensor(1479.0715, device='cuda:0')
    tensor(1444.9457, device='cuda:0')
    tensor(1404.3279, device='cuda:0')
    tensor(1406.7020, device='cuda:0')
    tensor(1401.9086, device='cuda:0')
    tensor(1411.5190, device='cuda:0')
    tensor(1412.3406, device='cuda:0')
    tensor(1407.0355, device='cuda:0')
    tensor(1425.3019, device='cuda:0')
    tensor(1471.7324, device='cuda:0')
    tensor(1458.9967, device='cuda:0')
    tensor(1458.5677, device='cuda:0')
    tensor(1421.5378, device='cuda:0')
    tensor(1424.3644, device='cuda:0')
    tensor(1399.3219, device='cuda:0')
    tensor(1426.1769, device='cuda:0')
    tensor(1428.2299, device='cuda:0')
    tensor(1394.2687, device='cuda:0')
    tensor(1450.3566, device='cuda:0')
    tensor(1429.1484, device='cuda:0')
    tensor(1409.4882, device='cuda:0')
    tensor(1401.9738, device='cuda:0')
    tensor(1408.9937, device='cuda:0')
    tensor(1431.1775, device='cuda:0')
    tensor(1446.2382, device='cuda:0')
    tensor(1389.8379, device='cuda:0')
    tensor(1380.1997, device='cuda:0')
    tensor(1434.7024, device='cuda:0')
    tensor(1413.0824, device='cuda:0')
    tensor(1375.3699, device='cuda:0')
    tensor(1456.1954, device='cuda:0')
    tensor(1448.0901, device='cuda:0')
    tensor(1411.6409, device='cuda:0')
    tensor(1381.5652, device='cuda:0')
    tensor(1434.2756, device='cuda:0')
    tensor(1439.8295, device='cuda:0')
    tensor(1403.3726, device='cuda:0')
    tensor(1388.3306, device='cuda:0')
    tensor(1420.6050, device='cuda:0')
    tensor(1438.3170, device='cuda:0')
    tensor(1422.2806, device='cuda:0')
    tensor(1403.1416, device='cuda:0')
    tensor(1455.1946, device='cuda:0')
    tensor(1393.4119, device='cuda:0')
    tensor(1429.4374, device='cuda:0')
    tensor(1388.5776, device='cuda:0')
    tensor(1402.1976, device='cuda:0')
    tensor(1419.8860, device='cuda:0')
    tensor(1418.9652, device='cuda:0')
    tensor(1420.8337, device='cuda:0')
    tensor(1414.4735, device='cuda:0')
    tensor(1408.0876, device='cuda:0')
    tensor(1456.0737, device='cuda:0')
    tensor(1406.4535, device='cuda:0')
    tensor(1384.9897, device='cuda:0')
    tensor(1414.2520, device='cuda:0')
    tensor(1413.4073, device='cuda:0')
    tensor(1400.8571, device='cuda:0')
    tensor(1380.0747, device='cuda:0')
    tensor(1377.0696, device='cuda:0')
    tensor(1379.1771, device='cuda:0')
    tensor(1399.6715, device='cuda:0')
    tensor(1441.7579, device='cuda:0')
    tensor(1399.0868, device='cuda:0')
    tensor(1456.7535, device='cuda:0')
    tensor(1401.8491, device='cuda:0')
    tensor(1413.5541, device='cuda:0')
    tensor(1429.7538, device='cuda:0')
    tensor(1406.7175, device='cuda:0')
    tensor(1453.6948, device='cuda:0')
    tensor(1402.9825, device='cuda:0')
    tensor(1411.7798, device='cuda:0')
    tensor(1441.6416, device='cuda:0')
    tensor(1404.1726, device='cuda:0')
    tensor(1432.4253, device='cuda:0')
    tensor(1413.1604, device='cuda:0')
    tensor(1401.4050, device='cuda:0')
    tensor(1425.7926, device='cuda:0')
    tensor(1413.5303, device='cuda:0')
    tensor(1405.1923, device='cuda:0')
    tensor(1450.2354, device='cuda:0')
    tensor(1407.7283, device='cuda:0')
    tensor(1436.4534, device='cuda:0')
    tensor(1384.6461, device='cuda:0')
    tensor(1398.5176, device='cuda:0')
    tensor(1438.2490, device='cuda:0')
    tensor(1457.0647, device='cuda:0')
    tensor(1426.2281, device='cuda:0')
    tensor(1419.3256, device='cuda:0')
    tensor(1399.1471, device='cuda:0')
    tensor(1423.1528, device='cuda:0')
    tensor(1397.5256, device='cuda:0')
    tensor(1415.1674, device='cuda:0')
    tensor(1407.4146, device='cuda:0')
    tensor(1452.3379, device='cuda:0')
    tensor(1446.0323, device='cuda:0')
    tensor(1441.5829, device='cuda:0')
    tensor(1407.5884, device='cuda:0')
    tensor(1420.2661, device='cuda:0')
    tensor(1431.3302, device='cuda:0')
    tensor(1410.5631, device='cuda:0')
    tensor(1417.2163, device='cuda:0')
    tensor(1437.8044, device='cuda:0')
    tensor(1412.6334, device='cuda:0')
    tensor(1436.4994, device='cuda:0')
    tensor(1418.1632, device='cuda:0')
    tensor(1420.6115, device='cuda:0')
    tensor(1439.8173, device='cuda:0')
    tensor(1414.1357, device='cuda:0')
    tensor(1394.1591, device='cuda:0')
    tensor(1401.5677, device='cuda:0')
    tensor(1402.8163, device='cuda:0')
    tensor(1418.9330, device='cuda:0')
    tensor(1441.8569, device='cuda:0')
    tensor(1423.7625, device='cuda:0')
    tensor(1399.2745, device='cuda:0')
    tensor(1393.4146, device='cuda:0')
    tensor(1401.2682, device='cuda:0')
    tensor(1424.5221, device='cuda:0')
    tensor(1394.0769, device='cuda:0')
    tensor(1366.8616, device='cuda:0')
    tensor(1410.2253, device='cuda:0')
    tensor(1399.7590, device='cuda:0')
    tensor(1407.2003, device='cuda:0')
    tensor(1418.5439, device='cuda:0')
    tensor(1460.2466, device='cuda:0')
    tensor(1429.7393, device='cuda:0')
    tensor(1385.3613, device='cuda:0')
    tensor(1438.1396, device='cuda:0')
    tensor(1390.1895, device='cuda:0')
    tensor(1463.1154, device='cuda:0')
    tensor(1409.0605, device='cuda:0')
    tensor(1412.3632, device='cuda:0')
    tensor(1437.3600, device='cuda:0')
    tensor(1395.8154, device='cuda:0')
    tensor(1365.6179, device='cuda:0')
    tensor(1441.3519, device='cuda:0')
    tensor(1386.6602, device='cuda:0')
    tensor(1423.1627, device='cuda:0')
    tensor(1410.9005, device='cuda:0')
    tensor(1390.1128, device='cuda:0')
    tensor(1435.8666, device='cuda:0')
    tensor(1444.9093, device='cuda:0')
    tensor(1387.8092, device='cuda:0')
    tensor(1444.0508, device='cuda:0')
    tensor(1418.8849, device='cuda:0')
    tensor(1390.7021, device='cuda:0')
    tensor(1424.4188, device='cuda:0')
    tensor(1444.6207, device='cuda:0')
    tensor(1423.9720, device='cuda:0')
    tensor(1408.1212, device='cuda:0')
    tensor(1435.5199, device='cuda:0')
    tensor(1439.0817, device='cuda:0')
    tensor(1428.3363, device='cuda:0')
    tensor(1420.5914, device='cuda:0')
    tensor(1456.4099, device='cuda:0')
    tensor(1443.0768, device='cuda:0')
    tensor(1404.6733, device='cuda:0')
    tensor(1383.9767, device='cuda:0')
    tensor(1411.3325, device='cuda:0')
    tensor(1444.2469, device='cuda:0')
    tensor(1422.6304, device='cuda:0')
    tensor(1406.2065, device='cuda:0')
    tensor(1426.7931, device='cuda:0')
    tensor(1442.3892, device='cuda:0')
    tensor(1475.6561, device='cuda:0')
    tensor(1426.5789, device='cuda:0')
    tensor(1421.5833, device='cuda:0')
    tensor(1430.1593, device='cuda:0')
    tensor(1438.6716, device='cuda:0')
    tensor(1432.8663, device='cuda:0')
    tensor(1440.8177, device='cuda:0')
    tensor(1453.5370, device='cuda:0')
    tensor(1414.8185, device='cuda:0')
    tensor(1409.4082, device='cuda:0')
    tensor(1443.3878, device='cuda:0')
    tensor(1410.9117, device='cuda:0')
    tensor(1450.1927, device='cuda:0')
    tensor(1439.9794, device='cuda:0')
    tensor(1455.7130, device='cuda:0')
    tensor(1408.6846, device='cuda:0')
    tensor(1370.5558, device='cuda:0')
    tensor(1411.1130, device='cuda:0')
    tensor(1425.2698, device='cuda:0')
    tensor(1415.9679, device='cuda:0')
    tensor(1426.5792, device='cuda:0')
    tensor(1430.0621, device='cuda:0')
    tensor(1429.0353, device='cuda:0')
    tensor(1436.3405, device='cuda:0')
    tensor(1415.4597, device='cuda:0')
    tensor(1398.7305, device='cuda:0')
    tensor(1412.2063, device='cuda:0')
    tensor(1396.1304, device='cuda:0')
    tensor(1392.3531, device='cuda:0')
    tensor(1447.8683, device='cuda:0')
    tensor(1408.5255, device='cuda:0')
    tensor(1443.4084, device='cuda:0')
    tensor(1392.5245, device='cuda:0')
    tensor(1424.6589, device='cuda:0')
    tensor(1386.9657, device='cuda:0')
    tensor(1404.4888, device='cuda:0')
    tensor(1411.2113, device='cuda:0')
    tensor(1418.5787, device='cuda:0')
    tensor(1425.8960, device='cuda:0')
    tensor(1429.8405, device='cuda:0')
    tensor(1439.3208, device='cuda:0')
    tensor(1435.7345, device='cuda:0')
    tensor(1378.0029, device='cuda:0')
    tensor(1435.4418, device='cuda:0')
    tensor(1424.1644, device='cuda:0')
    tensor(1433.0961, device='cuda:0')
    tensor(1409.1449, device='cuda:0')
    tensor(1435.2638, device='cuda:0')
    tensor(1427.7124, device='cuda:0')
    tensor(1399.4819, device='cuda:0')
    tensor(1431.6205, device='cuda:0')
    tensor(1450.2629, device='cuda:0')
    tensor(1408.6251, device='cuda:0')
    tensor(1441.3998, device='cuda:0')
    tensor(1424.9849, device='cuda:0')
    tensor(1433.3588, device='cuda:0')
    tensor(1412.4851, device='cuda:0')
    tensor(1411.2235, device='cuda:0')
    tensor(1418.6223, device='cuda:0')
    tensor(1390.9233, device='cuda:0')
    tensor(1391.8456, device='cuda:0')
    tensor(1389.3798, device='cuda:0')
    tensor(1441.3324, device='cuda:0')
    tensor(1425.0355, device='cuda:0')
    tensor(1428.2485, device='cuda:0')
    tensor(1414.7605, device='cuda:0')
    tensor(1451.6646, device='cuda:0')
    tensor(1421.8340, device='cuda:0')
    tensor(1419.5419, device='cuda:0')
    tensor(1429.1060, device='cuda:0')
    tensor(1424.3914, device='cuda:0')
    tensor(1463.5829, device='cuda:0')
    tensor(1442.8510, device='cuda:0')
    tensor(1418.9368, device='cuda:0')
    tensor(1417.0631, device='cuda:0')
    tensor(1423.2100, device='cuda:0')
    tensor(1418.9729, device='cuda:0')
    tensor(1384.8419, device='cuda:0')
    tensor(1415.7753, device='cuda:0')
    tensor(1446.4684, device='cuda:0')
    tensor(1367.5852, device='cuda:0')
    tensor(1400.4757, device='cuda:0')
    tensor(1425.3563, device='cuda:0')
    tensor(1433.1383, device='cuda:0')
    tensor(1422.0845, device='cuda:0')
    tensor(1415.9822, device='cuda:0')
    tensor(1424.4999, device='cuda:0')
    tensor(1373.3406, device='cuda:0')
    tensor(1412.9338, device='cuda:0')
    tensor(1413.8733, device='cuda:0')
    tensor(1384.5674, device='cuda:0')
    tensor(1439.4919, device='cuda:0')
    tensor(1423.0312, device='cuda:0')
    tensor(1413.1295, device='cuda:0')
    tensor(1444.2146, device='cuda:0')
    tensor(1433.5981, device='cuda:0')
    tensor(1446.2844, device='cuda:0')
    tensor(1436.2203, device='cuda:0')
    tensor(1388.2151, device='cuda:0')
    tensor(1410.3483, device='cuda:0')
    tensor(1428.2197, device='cuda:0')
    tensor(1467.3116, device='cuda:0')
    tensor(1387.5735, device='cuda:0')
    tensor(1418.7992, device='cuda:0')
    tensor(1389.2145, device='cuda:0')
    tensor(1441.4642, device='cuda:0')
    tensor(1430.0614, device='cuda:0')
    tensor(1425.9270, device='cuda:0')
    tensor(1379.5526, device='cuda:0')
    Time taken: 1.1767361164093018
    实验一消耗时间：0.083892822265625，实验二消耗时间：1.1767361164093018


### 练习 5.6.4

测量同时在两个GPU上执行两个矩阵乘法与在一个GPU上按顺序执行两个矩阵乘法所需的时间。提示：应该看到近乎线性的缩放。

**解答：** 

&emsp;&emsp;执行两个矩阵乘法并行在两个GPU上所需的时间通常会比在单个GPU上按顺序执行这两个操作要快得多。但实际的时间取决于矩阵的大小、硬件配置和算法实现。

&emsp;&emsp;但由于笔者只有一张卡，所以只做了在单个GPU上顺序执行两个矩阵乘法的实验。


```python
import torch
import time

# 创建两个随机矩阵
a = torch.randn(10000, 10000).cuda()
b = torch.randn(10000, 10000).cuda()

# 顺序计算
start_time = time.time()
c1 = torch.matmul(a, b)
c2 = torch.matmul(a, b)
end_time = time.time()
sequential_time = end_time - start_time

print(f"Sequential time: {sequential_time:.8f} seconds")
```

    Sequential time: 0.00299954 seconds

